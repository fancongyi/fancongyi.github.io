
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" href="favicon.ico" />
<link rel="icon" href="favicon.ico" type="image/x-icon" />
<title>Congyi Fan (èŒƒèªæ¯…)</title>
  
<style>
  body {
    font-family: "Times New Roman", Times, serif;
  }
  
  /* å…¨å±€ç¼©å°æ®µè½ä¸‹æ–¹é—´è· */
  p {
    margin-bottom: 6px;
    line-height: 1.5;
  }
  
  /* â†“ ç¼©å°åˆ—è¡¨é¡¹ä¸­çš„æ®µé—´è· â†“ */
  ul li p {
    margin-top: 4px;
    margin-bottom: 4px;
    line-height: 1.4;
  }
</style>

<style>
/* ç¼©å° publication åˆ—è¡¨é¡¹ä¹‹é—´çš„å‚ç›´é—´è· */
li p {
  margin-top: 4px;
  margin-bottom: 4px;
  line-height: 1.4;
}

/* å¯é€‰ï¼šè¿›ä¸€æ­¥æ§åˆ¶åˆ—è¡¨é¡¹æ•´ä½“å¤–è¾¹è·ï¼ˆå¦‚æœ <li> æœ¬èº«æœ‰ marginï¼‰ */
li {
  margin-bottom: 6px; /* æ§åˆ¶æ¯é¡¹ä¹‹é—´çš„é—´è· */
}
</style>
  
<style>
  /* æŠŠæ‰€æœ‰ <b> å†…çš„é“¾æ¥éƒ½è®¾æˆæ©™è‰² */
  b a {
    color: #224b8d;       
    text-decoration: underline;  /* ä¿ç•™ä¸‹åˆ’çº¿ */
  }
  /* ä½ è¿˜å¯ä»¥å®šä¹‰ hoverã€visited ç­‰çŠ¶æ€ */
  b a:hover {
    color: #3069C6;       /* é¼ æ ‡æ‚¬åœæ—¶ç¨å¾®äº®ä¸€ç‚¹ */
  }
  b a:visited {
    color: #843F9F;       /* è®¿é—®è¿‡åæ¢æˆæ·±ç´« */
  }
</style>
  
<style>
  a.custom-link {
    color: #224b8d;
    text-decoration: underline;
  }
  a.custom-link:hover {
    color: #3069C6;
  }
  a.custom-link:visited {
    color: #843F9F;
  }
</style>

<style>
#layout-menu {
  background-color: #f8f9fa; /* æµ…ç°èƒŒæ™¯ */
  padding: 15px;
  border-right: 1px solid #ccc;
  width: 56px;
}
#layout-content {
  padding: 25px;
  background-color: #ffffff;
}
</style>
  
<style>
#tlayout {
  margin: 0 auto;
  max-width: 1500px;
  border-collapse: collapse;
}
</style>

<style>
h1, h2, h3, h4, h5, h6 {
  margin-top: 16px;
  margin-bottom: 6px;
}

li {
  margin-bottom: 4px;
}
</style>


</head>

<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="https://scholar.google.com/citations?user=bBNwgX8AAAAJ">Scholar</a></div>
<div class="menu-item"><a href="https://github.com/fancongyi">GitHub</a></div>
<!-- <div class="menu-item"><a href="https://www.linkedin.com/in/ziyang-ma">Linkedin</a></div> -->
<!-- <div class="menu-item"><a href="https://blog.csdn.net/weixin_45019478">Blog</a></div> -->
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Congyi Fan (èŒƒèªæ¯…)</h1>
</div>
<table class="imgtable">
  <tr>
    <td>
      <a href="index.html" style="text-decoration: none;">
        <img src="profile.png" alt="Congyi Fan's Avatar" width="150px" height="150px" style="border-radius: 8px; box-shadow: 2px 2px 6px rgba(0,0,0,0.2);" />
      </a>&nbsp;
    </td>
    <td align="left">
      <p>
        Undergraduate Student,<br />
        Harbin Engineering University,<br />
        Harbin, Heilongjiang Province, China.<br />
        <a href="mailto:fancongyi@hrbeu.edu.cn">fancongyi@hrbeu.edu.cn</a>
      </p>
    </td>
  </tr>
</table>



<h2>Biography</h2>
<p>HiğŸ‘‹ nice to meet you!</p>
<p>Currently I am an undergraduate student majoring in Computer Science and Technology (Information Security) at Harbin Engineering University (HEU), serving as a research assistant to Associate Professor Guan Jian.</p>
<p>My research is jointly supervised by Associate Professor Guan Jian at HEU, Researcher Pengming Feng from the State Key Laboratory of Space-Ground Integrated Information Technology (SGIIT), and Dr. Qiaoxi Zhu from the University of Technology Sydney.<p>
<p>My research is guided by the aspiration to advance scientific and technological progress. My recent work focuses on <b>Generative Models, 3D Vision and Multimodal</b>. If you are also interested, please feel free to contact me.</p>

<h3>Education</h3>
<ul>
<!-- <li><p>Ph.D., Computer Science and Engineering, Shanghai Jiao Tong University, 2022.09-Now</p>
</li>
<li><p>Ph.D., Computer Science and Engineering, Nanyang Technological University, 2022.09-Now</p>
</li> -->
<li><p>B.E., Information Security, Harbin Engineering University, 2022.09-Now</p>
</li>
</ul>

<h3>Interests</h3>
<ul>
<li><p>Generative Models</p>
</li>
<li><p>3D Vision</p>
</li>
<li><p>Multimodal</p>
<!-- </li>
<li><p>Multimodal Reasoning</p> -->
</li>
</ul>

<style>
/* NEWSå®¹å™¨æ ·å¼ */
.news-container {
    max-height: 400px;       /* è®¾ç½®æœ€å¤§é«˜åº¦ */
    overflow-y: auto;        /* å‚ç›´æ–¹å‘è‡ªåŠ¨æ˜¾ç¤ºæ»šåŠ¨æ¡ */
    padding-right: 15px;     /* ä¸ºæ»šåŠ¨æ¡é¢„ç•™ç©ºé—´ */
    margin: 20px 0;          /* æ·»åŠ å¤–è¾¹è· */
    border: 1px solid #eee;  /* å¯é€‰è¾¹æ¡† */
    border-radius: 8px;      /* åœ†è§’ */
}

/* è‡ªå®šä¹‰æ»šåŠ¨æ¡æ ·å¼ */
.news-container::-webkit-scrollbar {
    width: 6px;              /* æ»šåŠ¨æ¡å®½åº¦ */
}
.news-container::-webkit-scrollbar-track {
    background: #f1f1f1;     /* è½¨é“é¢œè‰² */
    border-radius: 3px;
}
.news-container::-webkit-scrollbar-thumb {
    background: #888;        /* æ»‘å—é¢œè‰² */
    border-radius: 3px;
}
.news-container::-webkit-scrollbar-thumb:hover {
    background: #555;        /* é¼ æ ‡æ‚¬åœé¢œè‰² */
}

/* è°ƒæ•´åŸæœ‰æ ·å¼ */
.news-container h3 {
    margin: 15px 0;
    position: sticky;        /* å›ºå®šæ ‡é¢˜ */
    top: 0;
    background: white;
    z-index: 1;
    padding: 10px 0;
}
.news-container ul {
    margin: 0;
    padding: 0 0 15px 25px;  /* è°ƒæ•´åˆ—è¡¨å†…è¾¹è· */
}
</style>

<!-- HTMLç»“æ„ -->
<div class="news-container">
<h3>NEWS</h3>
<!-- <img src="pic/new_gif.gif" alt="alt text"/> -->
<ul>
<li><p>[2025.05] 1 papers was accpeted by ICCV 2025. [<a href="https://news.hrbeu.edu.cn/info/1015/86542.htm">News</a>]</p></li>
<li><p>[2025.04] Selected as a Pioneer in Innovation and Entrepreneurship (Top 0.03%).[<a href="https://qihang.hrbeu.edu.cn/2024/0427/c960a324682/page.htm">News</a>]</p></li>
<li><p>[2025.04] Received Outstanding Final Report Award for the National Undergraduate Training Program for Innovation and Entrepreneurship.</p></li>
<li><p>[2025.03] New paper released! Align Your Rhythm: Generating Highly Aligned Dance Poses with Gating-Enhanced Rhythm-Aware Feature Representation.</p></li>
<li><p>[2024.12] Awarded First Prize at the 9th National Youth Artificial Intelligence Innovation and Entrepreneurship Conference.</p></li>
<li><p>[2024.11] FastDrag was successfully selected as an Outstanding Achievement in the First Academic Annual Conference of Harbin Engineering University. [<a href="https://cstc.hrbeu.edu.cn/2024/1105/c3687a331657/page.htm">News</a>]</p></li>
<li><p>[2025.05] 1 papers was accpeted by NeurIPS 2024. [<a href="https://cstc.hrbeu.edu.cn/2024/0929/c3687a330582/page.htm">News</a>]</p></li>
<li><p>[2024.09] I have received a letter of appreciation from the Heilongjiang Provincial Department of Education.</p></li>
<li><p>[2024.07] As the project leader, successfully led the application for the 2024 National Undergraduate Training Program for Innovation and Entrepreneurship (Top 4.16%).</p></li>
<li><p>[2023.09] Participated in the 2023 Forum on Technological Innovation for the Maritime Power Strategy.</p></li>
<li><p>[2023.08] Participated in the 18th National Conference on Computer-Supported Cooperative Work and Social Computing.</p></li>
<!-- <li><p> </p></li>
<li><p> </p></li>
<li><p> </p></li>
<li><p> </p></li>
<li><p> </p></li>
<li><p> </p></li> -->
</ul>
</div>


<h2>Research</h2>
<h3>Selected Publications</h3>
<p>Thanks to all the collaborators for their great work!</p>
<p>Check out <b><a href="https://scholar.google.com/citations?user=bBNwgX8AAAAJ">Google Scholar</a></b> for more information.</p>

<!-- <p><b>Speech, Language, Audio, Music Processing with SSL</b></p> -->
<ul>

<li><p>
<i><span style="color:#224b8d"><b>Congyi Fan</b></span>, Jian Guan, Xuanjia Zhao, Dongli Xu, Youtian Lin, Tong Ye, Haiwei Pan, Pengming Feng
.</i><br>
<b>Align Your Rhythm: Generating Highly Aligned Dance Poses with Gating-Enhanced Rhythm-Aware Feature Representation.</b><br>
in <i>IEEE/CVF International Conference on Computer Vision (ICCV), 2025.</i> [<a href="https://arxiv.org/abs/2503.17340">arXiv</a>][<a href="https://danceba.github.io/">Demo</a>][<a href="https://github.com/fancongyi/Danceba">Code</a>]<br>
</p></li>
<li><p>

<i>Xuanjia Zhao, Jian Guan, <span style="color:#224b8d"><b>Congyi Fan</b></span>, Dongli Xu, Youtian Lin, Haiwei Pan, Pengming Feng
.</i><br>
<b>FastDrag: Manipulate Anything in One Step.</b><br>
in <i>Annual Conference on Neural Information Processing Systems (NeurIPS), 2024.</i> [<a href="https://arxiv.org/abs/2405.15769">arXiv</a>][<a href="https://fastdrag-site.github.io/">Demo</a>][<a href="https://github.com/XuanjiaZ/FastDrag">Code</a>]<br>
</p></li>
<li><p>

<i><span style="color:#224b8d"><b>Congyi Fan*</b></span>, Shitong Fan*, Feiyang Xiao, Wenbo Wang, Xinyi Che, Qiaoxi Zhu, Jian Guan
.</i><br>
<b>GISP@HEU's Submission to the DCASE 2025 Challenge: Stereo SELD Task.</b><br>
in <i>Dcase 2025 Technical Report.</i> [<a href="https://dcase.community/documents/challenge2025/technical_reports/DCASE2025_Guan_77_t3.pdf">Paper</a>] <br>
</p></li>

</ul>

<!-- <p>Note: * indicates the corresponding author.</p> -->
<!-- <p><a href="https://scholar.google.com/citations?user=4RZnXGMAAAAJ&hl=zh-CN&oi=ao">Full list of publications in Google Scholar</a>.</p> -->

<h3>Experiences</h3>
<p>Research Intern, Guangdong Key Laboratory of New Security and Intelligence Technology, HITSZ, 2024.07-2024.11</p>
<ul>
<!-- <li><p>Investigate full-duplex modeling for speech interaction and dialog system.</p>
</li> -->
<li><p>Led by <a href="https://faculty.hitsz.edu.cn/wangxuan" class="custom-link">Xuan Wang</a> and Supervised by <a href="http://csen.hitsz.edu.cn/info/1021/1962.htm" class="custom-link">Shuhan Qi</a>.</p>
</li>
</ul> 
<p>Research Intern, State Key Laboratory of Space-Ground Integrated Information Technology (SGIIT), National Key Laboratory, 2025.06-now</p>
<ul>
<!-- <li><p>Investigate emotional speech dialogue system with large language model.</p>
</li> -->
<li><p>Supervised by <a href="https://scholar.google.com/citations?user=O51mMKgAAAAJ" class="custom-link">Pengming Feng</a> and <a href="https://scholar.google.com/citations?user=wf60G1sAAAAJ" class="custom-link">Jian Guan</a>
</li>
</ul> 
</ul>

<h3>Academic Service</h3>
<!-- <p><b>Organizing Committee / Chair</b></p>
<ul>
<li><p>Multimodal Emotion Recognition Challenge (MER24) @ACM Multimedia MRAC24 Workshop</p>
</li>
</ul> -->
<p><b>Conference Reviewer / TPC Member</b></p>
<ul>
<li><p>IEEE International Conference on Multimedia & Expo (ICME) 2024, 2025</p>
</li>    
<li><p>Chinese Conference on Pattern Recognition and Computer Vision (PRCV) 2025</p>
</li>
</ul>
<!-- <p><b>Journal Reviewer</b></p>
<ul>
<li><p>IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCAVT)</p>
</li> -->
</ul>
<!-- <p><a href="https://publons.com/researcher/3034188/xiuze-zhou/">More details in Publons</a></p> -->

<!-- <h2>Open-Source Projects</h2>
<h3>Projects</h3>
<p><b>SLAM-LLM</b>[<a href="https://github.com/ddlBoJack/SLAM-LLM">GitHub</a>]</p>
<ul>
<li><p>SLAM-LLM is a deep learning toolkit that allows researchers and developers to train custom multimodal large language model (MLLM), focusing on Speech, Language, Audio, Music processing.</p>
</li>
</ul>
<p><b>FunAudioLLM</b>[<a href="https://github.com/FunAudioLLM">GitHub</a>][<a href="https://fun-audio-llm.github.io/pdf/FunAudioLLM.pdf">Techinical Report</a>][<a href="https://huggingface.co/FunAudioLLM">HuggingFace</a>][<a href="https://fun-audio-llm.github.io/">Demo</a>]</p>
<ul>
<li><p><b>SenseVoice</b> is a speech foundation model with multiple speech understanding capabilities.[<a href="https://github.com/FunAudioLLM/SenseVoice">GitHub</a>][<a href="https://www.modelscope.cn/models/iic/SenseVoiceSmall">ModelScope</a>]</p>
</li>
<li><p><b>CosyVoice</b> is a multi-lingual large voice generation model.[<a href="https://github.com/FunAudioLLM/CosyVoice">GitHub</a>][<a href="https://www.modelscope.cn/studios/iic/CosyVoice-300M">ModelScope</a>]</p>
</li>
</ul>
<p><b>emotion2vec series</b>[<a href="https://github.com/ddlBoJack/emotion2vec">GitHub</a>][<a href="https://arxiv.org/abs/2312.15185">emotion2vec(ACL2024)</a>][<a href="https://huggingface.co/emotion2vec">HuggingFace</a>][<a href="https://modelscope.cn/models?name=emotion2vec">ModelScope</a>]</p>
<ul>
<li><p><b>emotion2vec</b> is the first universal speech emotion representation model.</p>
</li>
<li><p><b>emotion2vec+</b> is a series of foundational models for speech emotion recognition (SER).</p>
</li>
</ul>
<p><b>MAP-Neo series</b>[<a href="https://github.com/multimodal-art-projection/MAP-NEO">GitHub</a>][<a href="https://arxiv.org/abs/2405.19327">Techinical Report</a>][<a href="https://huggingface.co/collections/m-a-p/neo-models-66395a5c9662bb58d5d70f04">HuggingFace</a>]</p>
<ul>
<li><p><b>MAP-Neo</b> is a series of fully open-sourced large language models.</p>
</li>
<li><p><b>Matrix</b> is the pretraining data and data processing pipeline for MAP-Neo.[<a href="https://huggingface.co/datasets/m-a-p/Matrix">Dataset</a>]</p>
</li>
</ul> -->

<!-- <h2>Accomplishments</h2>
<h3>Awards</h3>
<ul>
<li><p>SPS Travel Grant, IEEE, 2024.02</p>
</li>
<li><p>Best Presentation Award in Student Forum, the 18th National Conference on Man-Machine Speech Communication (NCMMSC), 2023.12</p>
</li>
<li><p>Interspeech Best Student Paper Shortlist, ISCA, 2023.08</p>
</li>
<li><p>Excellent Graduate, Department of Education, Shandong Province, China, 2022.06</p>
</li>
<li><p>"Intelligent Pedestal" Scholarship, Huawei, 2021.12</p>
</li>
<li><p>SIGMM Student Travel Grant, ACM, 2021.11</p>
</li>
<li><p>National Scholarship, Ministry of Education, China, 2021.10</p>
</li>
</ul> -->

<h3>Competitions</h3>
<ul>
<li><p>Ranked 7th globally in Task 1 and 4th globally in Task 2 of the EEG-Music Emotion Recognition challenge at ICASSP 2024 Grand Challenge, 2024.01.</p></li>
<li><p>1st in 9th National Youth Artificial Intelligence Innovation and Entrepreneurship Conference, 2024.12. </p></li>
<li><p>3rd in Northeast Region National Undergraduate Information Security Competition, 2024.07.</p></li>
<li><p>Honorable Mention in American Undergraduate Mathematical Contest in Modeling, 2024.05.</p></li>
<li><p>Heilongjiang Province 1st in National Undergraduate Mathematical Contest in Modeling, 2023.11.</p></li>
(Total of 2 national awards and 7 provincial awards)
</ul>

<!-- <h3>Activities</h3>
<ul>
<li><p>Invited Talk: Towards Interactive Speech Language Model, Nvidia, 2024.10</p></li>
<li><p>Invited Talk: Towards Interactive Speech Language Model, The Hong Kong University of Science and Technology(HKUST), 2024.8</p></li>
<li><p>Invited Talk: Speech & Audio Understanding Based on SSL and LLM, Nvidia, 2024.6</p></li>
<li><p>Invited Talk: INTERSPEECH 2023 Pre-presentation, SpeechHome, 2023.07</p></li>
<li><p>Invited Talk: Towards More Realistic, Powerful, and Accurate Speech-based Self-Supervised Learning </a>, The Renmin University of China(RUC), 2023.5</p></li>
<li><p>PhD Debate Towards AIGC, AI TIME, 2023.1</p></li>
<li><p>[<a href="https://www.bilibili.com/video/BV1iV4y137RC">Invited Talk</a>]: How to conduct audio-driven talking head? An introduction and solution sharing, Datawhale, 2022.11</p></li>
<li><p>Member of <a href="https://datawhale.club/">Datawhale</a>, 2022.09-Now</p></li>
<li><p>Teaching Assistant, Computer Science and Technology, Shandong University, 2021.03-2021.06</p></li>
<li><p>Member of Elite Class, Computer Science and Technology, Shandong University, 2020.09-2022.06</p></li>
</ul> -->

<!-- <p><br><a href="cv/cv.pdf">A brief cv</a>.</p> -->
<center>
<a href="https://clustrmaps.com/site/1bonu" title="Visit tracker"><img src="https://clustrmaps.com/map_v2.png?cl=ffffff&w=300&t=tt&d=6yrMMIqIUJ1GT9nydxBO8cdtx5nw8iVTjA_d5hAcBcY&co=2d78ad&ct=ffffff" /></a>
</center>
</td>
</tr>
</table>
</body>
</html>
