
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" href="favicon.ico" />
<link rel="bookmark" href="favicon.ico" type="image/x-icon"　/>
<title>Ziyang Ma (马子阳)</title>
</head>

<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="https://scholar.google.com/citations?user=4RZnXGMAAAAJ">Scholar</a></div>
<div class="menu-item"><a href="https://github.com/ddlBoJack">GitHub</a></div>
<div class="menu-item"><a href="https://www.linkedin.com/in/ziyang-ma">Linkedin</a></div>
<div class="menu-item"><a href="https://blog.csdn.net/weixin_45019478">Blog</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Ziyang Ma (马子阳)</h1>
</div>
<table class="imgtable"><tr><td>
<a href="index.html"><img src="pic/avatar.jpeg" alt="alt text" width="120px" height="160px" /></a>&nbsp;</td>
<td align="left"><p>Ph.D. student,<br />
Shanghai Jiao Tong University,<br />
800 Dongchuan RD. Minhang District, Shanghai, China.<br /> 
<a href="mailto:zym.22@sjtu.edu.cn">zym.22@sjtu.edu.cn</a><br /> 
Nanyang Technological University,<br />
50 Nanyang Ave, Singapore 639798.<br />
<a href="mailto:ziyang012@e.ntu.edu.sg">ziyang012@e.ntu.edu.sg</a></p>
</td></tr></table>


<h2>Biography</h2>
<p>Hi👋 nice to meet you!</p>
<p>Currently I am within the Joint Ph.D. Programme of Shanghai Jiao Tong University (SJTU) and Nanyang Technological University (NTU), co-supervised by Prof. <a href="https://chenxie95.github.io/">Xie Chen</a> from SJTU and Prof. <a href="https://aseschng.github.io/intro1.html">Chng Eng Siong</a> from NTU. I am also a member in <a href="https://x-lance.sjtu.edu.cn/">Cross Media (<b>X-</b>) <b>Lan</b>guage Intelligen<b>ce</b> Lab (X-LANCE)</a>, working closely with Prof. <a href="https://x-lance.sjtu.edu.cn/members/kai_yu">Kai Yu</a>. As the first Ph.D. supervised by Prof. Chen, I will try my best in the next five exciting years! 💪</p>
<p>I was a research assistant at <a href="https://ilearn.qd.sdu.edu.cn/"><b>I</b>nte<b>L</b>ligent m<b>e</b>di<b>a</b> <b>r</b>esearch ce<b>n</b>ter (iLearn)</a>, working closely with Prof. <a href="https://xuemengsong.github.io/">Xuemeng Song</a> and <a href="https://liqiangnie.github.io/">Liqiang Nie</a> during my undergraduate years.<p>
<p>My research usually follows the <a href="https://wikipedia.org/wiki/KISS_principle">KISS</a> philosophy. My recent work focuses on speech, language, audio and music processing with Self-Supervised Learning (SSL) and Large Language Model (LLM). If you are also interested, please feel free to contact me.</p>

<h3>Education</h3>
<ul>
<li><p>Ph.D., Computer Science and Engineering, Shanghai Jiao Tong University, 2022.09-Now</p>
</li>
<li><p>Ph.D., Computer Science and Engineering, Nanyang Technological University, 2022.09-Now</p>
</li>
<li><p>B.E., Computer Science and Technology, Shandong University, 2018.09-2022.06</p>
</li>
</ul>

<h3>Interests</h3>
<ul>
<li><p>Self-Supervised Learning</p>
</li>
<li><p>Speech and Audio Processing</p>
</li>
<li><p>Natural Language Processing</p>
</li>
<li><p>Multimodal Reasoning</p>
</li>
</ul>

<style>
/* NEWS容器样式 */
.news-container {
    max-height: 400px;       /* 设置最大高度 */
    overflow-y: auto;        /* 垂直方向自动显示滚动条 */
    padding-right: 15px;     /* 为滚动条预留空间 */
    margin: 20px 0;          /* 添加外边距 */
    border: 1px solid #eee;  /* 可选边框 */
    border-radius: 8px;      /* 圆角 */
}

/* 自定义滚动条样式 */
.news-container::-webkit-scrollbar {
    width: 6px;              /* 滚动条宽度 */
}
.news-container::-webkit-scrollbar-track {
    background: #f1f1f1;     /* 轨道颜色 */
    border-radius: 3px;
}
.news-container::-webkit-scrollbar-thumb {
    background: #888;        /* 滑块颜色 */
    border-radius: 3px;
}
.news-container::-webkit-scrollbar-thumb:hover {
    background: #555;        /* 鼠标悬停颜色 */
}

/* 调整原有样式 */
.news-container h3 {
    margin: 15px 0;
    position: sticky;        /* 固定标题 */
    top: 0;
    background: white;
    z-index: 1;
    padding: 10px 0;
}
.news-container ul {
    margin: 0;
    padding: 0 0 15px 25px;  /* 调整列表内边距 */
}
</style>

<!-- HTML结构 -->
<div class="news-container">
<h3>NEWS</h3>
<!-- <img src="pic/new_gif.gif" alt="alt text"/> -->
<ul>
<li><p>[2025.5] <img src="pic/new_gif.gif" alt="alt text"/> Check out our MMAR, a new benchmark designed to evaluate the deep reasoning capabilities of Audio-Language Models (ALMs).[<a href="https://arxiv.org/abs/2505.13032">arXiv</a>][<a href="https://www.youtube.com/watch?v=Dab13opIGqU">Demo</a>][<a href="https://github.com/ddlBoJack/MMAR">GitHub</a>][<a href="https://huggingface.co/datasets/BoJack/MMAR">Benchmark</a>] </p></li>
<li><p>[2025.5] 1 papers was accpeted by ISCA INTERSPEECH2025. </p></li>
<li><p>[2025.5] 5 papers were accpeted by ACL2025. </p></li>
<li><p>[2025.4] 1 paper was accpeted by IEEE TASLP.</p></li>
<li><p>[2025.3] 2 papers were accpeted by ICME2025. </p></li>
<li><p>[2025.3] 🔥 Check out our <a href="https://arxiv.org/abs/2503.01710">Spark-TTS</a> (along with BiCodec and VoxBox dataset), a LLM-based controllable TTS with both voice cooing and generation abilities. </p></li>
<li><p>[2025.1] Check out our <a href="https://arxiv.org/abs/2501.07246">Audio-CoT</a>, the first work to explore chain-of-thought reasoning in large audio language model (LALM). </p></li>
<li><p>[2025.1] <a href="https://github.com/X-LANCE/SLAM-LLM/blob/main/examples/s2s/README.md">Full reproduction</a> (including all data preparation, model training, inference and checkpoints) for <a href="https://arxiv.org/abs/2412.15649">SLAM-Omni</a> has been supported! </p></li>
<li><p>[2025.1] <a href="https://arxiv.org/abs/2404.06393">MUPT</a> was accpeted by ICLR2025.</p></li>
<li><p>[2025.1] <a href="https://arxiv.org/abs/2408.02622">LSLM</a>, <a href="https://arxiv.org/abs/2402.08846">SLAM-ASR</a> and <a href="https://arxiv.org/abs/2401.07333">ELLA-V</a> have been selected for Oral presentation at AAAI2025. </p></li>
<li><p>[2024.12] 3 papers were accpeted by ICASSP2025.</p></li>
<li><p>[2024.12] 4 papers were accpeted by AAAI2025.</p></li>
<li><p>[2024.10] Check out our <a href="https://arxiv.org/abs/2410.09503">SLAM-AAC</a>, a new member of <a href="https://github.com/ddlBoJack/SLAM-LLM">SLAM-LLM</a> family with SOTA audio captioning performance.</p></li>
<li><p>[2024.10] 1 paper was accpeted by IEEE TASLP.</p></li>
<li><p>[2024.10] Check out our <a href="https://arxiv.org/abs/2410.06885">F5-TTS</a>, a bilingual DiT-based TTS model with flow-matching!</p></li>
<li><p>[2024.8] 1 paper was accpeted by IEEE TMM.</p></li>
<li><p>[2024.8] 2 papers were accpeted by IEEE SLT2024.</p></li>
<li><p>[2024.7] <a href="https://arxiv.org/abs/2404.04167">Chinese Tiny LLM</a> was accepted by the 1st Conference on Language Modeling (COLM). </p></li>
<li><p>[2024.7] <a href="https://arxiv.org/abs/2404.17113">MER24 Baseline Paper</a> was accpeted by MRAC24 Workshop@ACM Multimedia.</p></li>
<li><p>[2024.7] Check out <a href="https://arxiv.org/abs/2407.04051">FunAudioLLM</a> family, including a speech understanding model <a href="https://github.com/FunAudioLLM/SenseVoice">SenseVoice</a> and a speech generation model <a href="https://github.com/FunAudioLLM/CosyVoice">CosyVoice</a>. </p></li>
<li><p>[2024.6] We organize <a href="http://www.iscslp2024.com/LLM">Speech Processing in LLM Era @ISCSLP 2024 Special Session</a> which has been open for submission. </p></li>
<li><p>[2024.6] 4 papers were accpeted by ISCA INTERSPEECH2024.</p></li>
<li><p>[2024.5] <a href="https://github.com/ddlBoJack/SLAM-LLM">SLAM-LLM</a>, a toolkit focusing on speech, language, audio, music processing with LLM, has been released! </p></li>
<li><p>[2024.5] <a href="https://arxiv.org/abs/2312.15185">emotion2vec</a> and <a href="https://arxiv.org/abs/2402.16153">ChatMusician</a> were accepted by ACL 2024 Findings. </p></li>
<li><p>[2024.5] <a href="https://arxiv.org/abs/2402.01591">BAT</a> was accepted by ICML 2024. </p></li>
<li><p>[2024.4] MER24 Challenge@IJCAI and MRAC24 Workshop@ACM Multimedia are coming! [<a href="https://arxiv.org/abs/2404.17113">Baseline Paper</a>][<a href="https://github.com/zeroQiaoba/MERTools/tree/master/MER2024">Baseline Code</a>][<a href="https://zeroqiaoba.github.io/MER2024-website/">Challenge Homepage</a>]</li>
<li><p>[2024.4] <a href="https://arxiv.org/abs/2401.03497">EAT</a> was accepted by IJCAI 2024. </p></li>
<li><p>[2024.3] We won the 1st place in Categorical Emotion Recognition at <a href="https://www.odyssey2024.org/emotion-recognition-challenge">Odyssey 2024 Emotion Recognition Challenge</a>.[<a href="https://arxiv.org/abs/2405.20064">Technical Report</a>] </p></li>
<li><p>[2024.1] Check out our <a href="https://github.com/cwx-worst-one/EAT">Repo</a> for EAT,  a new audio representation model with both effectiveness and efficiency. </p></li>
<li><p>[2023.12] Check out our <a href="https://github.com/ddlBoJack/emotion2vec">Repo</a> for emotion2vec, the first universal speech emotion representation model. </p></li>
<li><p>[2023.12] 4 papers were accpeted by IEEE ICASSP2024.</p></li>
<li><p>[2023.9] Check out our <a href="https://github.com/yanghaha0908/FastHuBERT">Repo</a> for Fast-HuBERT. We accelerate HuBERT pre-training in 5.2X speedup without performance drop. </p></li>
<li><p>[2023.9] 2 papers were accpeted by IEEE ASRU2023.</p></li>
<li><p>[2023.8] <a href="https://arxiv.org/abs/2211.07321">MT4SSL</a> was nominated in <font color=red>ISCA Interspeech Best Student Paper Shortlist</font>.</p></li>
<li><p>[2023.5] 4 papers were accpeted by ISCA INTERSPEECH2023.</p></li>
<li><p>[2023.2] 2 papers were accpeted by IEEE ICASSP2023.</p></li>
<li><p>[2022.11] Check out our <a href="https://github.com/ddlBoJack/MT4SSL">Repo</a> for MT4SSL, a multi-task learning framework for self-supervised learning.</p></li>
<li><p>[2022.09] We won 3rd place in <a href="http://www.aiwin.org.cn/competitions/69">Avatar Track of AIWIN</a>, held by WAIC2022.[<a href="https://mp.weixin.qq.com/s/3wuZG4I4PbXW9WmAfuVFwQ">Report</a>][<a href="https://www.bilibili.com/video/BV1iV4y137RC">Invited Talk</a>]</p></li>
</ul>
</div>


<h2>Research</h2>
<h3>Selected Publications</h3>
<p>Thanks to all the collaborators for their great work!</p>
<p>Check out <a href="https://scholar.google.com/citations?user=4RZnXGMAAAAJ">Google Scholar</a> for more information.</p>

<p><b>Speech, Language, Audio, Music Processing with SSL</b></p>
<ul>
<li><p>
<i>Haina Zhu, Yizhi Zhou, Hangting Chen, Jianwei Yu, <b>Ziyang Ma</b>, Rongzhi Gu, Yi Luo, Wei Tan, Xie Chen
.</i><br>
<b><a href="https://arxiv.org/abs/2501.01108">MuQ: Self-Supervised Music Representation Learning with Mel Residual Vector Quantization.</a></b><br>
in <i>arXiv, 2025.</i><br>
</p></li>
<li><p>
<i><b>Ziyang Ma*</b>, Mingjie Chen*, Hezhao Zhang, Zhisheng Zheng, Wenxi Chen, Xiquan Li, Jiaxin Ye, Xie Chen, Thomas Hain
.</i><br>
<b><a href="https://arxiv.org/abs/2406.07162">EmoBox: Multilingual Multi-corpus Speech Emotion Recognition Toolkit and Benchmark.</a></b><br>
<font color=red>Oral</font> in <i>INTERSPEECH, 2024.</i><br>
</p></li>
<li><p>
<i>Wenxi Chen, Yuzhe Liang, <b>Ziyang Ma</b>, Zhisheng Zheng, Xie Chen.</i><br>
<b><a href="https://arxiv.org/abs/2401.03497">EAT: Self-Supervised Pre-Training with Efficient Audio Transformer.</a></b><br>
in <i>International Joint Conference on Artificial Intelligence (IJCAI), 2024.</i><br>
</p></li>
<li><p>
<i><b>Ziyang Ma</b>, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, Shiliang Zhang, Xie Chen.</i><br>
<b><a href="https://arxiv.org/abs/2312.15185">emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation.</a></b><br>
in <i>the annual meeting of the Association for Computational Linguistic (ACL), Findings, 2024.</i><br>
</p></li>
<li><p>
<i>Guanrou Yang, <b>Ziyang Ma</b>, Zhisheng Zheng, Yakun Song, Zhikang Niu, Xie Chen.</i><br>
<b><a href="https://arxiv.org/abs/2309.13860">Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning.</a></b><br>
in <i>IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2023.</i><br>
</p></li>
<li><p>
<i><b>Ziyang Ma</b>, Zhisheng Zheng, Guanrou Yang, Yu Wang, Chao Zhang, Xie Chen.</i><br>
<b><a href="https://arxiv.org/abs/2306.08920">Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation.</a></b><br>
in <i>INTERSPEECH, 2023.</i><br>
</p></li>
<li><p>
<i><b>Ziyang Ma</b>, Zhisheng Zheng, Changli Tang, Yujin Wang, Xie Chen.</i><br>
<b><a href="https://arxiv.org/abs/2211.07321">MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets.</a></b><br>
<font color=red>Oral & Best Student Paper Shortlist</font> in <i>INTERSPEECH, 2023.</i><br>
</p></li>
</ul>

<p><b>Speech, Language, Audio, Music Processing with LLM</b></p>
<ul>
<li><p>
<i><b>Ziyang Ma</b>, Guanrou Yang, Yifan Yang, Zhifu Gao, Jiaming Wang, Zhihao Du, Fan Yu, Qian Chen, Siqi Zheng, Shiliang Zhang, Xie Chen.</i><br>
<b><a href="https://arxiv.org/abs/2402.08846">Speech Recognition Meets Large Language Model: Benchmarking, Models, and Exploration.</a></b><br>
<font color=red>Oral</font> in <i>the Annual AAAI Conference on Artificial Intelligence (AAAI), 2025.</i><br>
</p></li>
<li><p>
<i>Yexing Du*, <b>Ziyang Ma*</b>, Yifan Yang, Keqi Deng, Xie Chen, Bo Yang, Yang Xiang, Ming Liu, Bing Qin.</i><br>
<b><a href="https://arxiv.org/abs/2409.19510">CoT-ST: Enhancing LLM-based Speech Translation with Multimodal Chain-of-Thought.</a></b><br>
in <i>arXiv, 2024.</i><br>
</p></li>
<li><p>
<i>Wenxi Chen*, <b>Ziyang Ma*</b>, Xiquan Li, Xuenan Xu, Yuzhe Liang, Zhisheng Zheng, Kai Yu, Xie Chen.</i><br>
<b><a href="https://arxiv.org/abs/2410.09503">SLAM-AAC: Enhancing Audio Captioning with Paraphrasing Augmentation and CLAP-Refine through LLMs.</a></b><br>
in <i>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2025.</i><br>
</p></li>
<li><p>
<i>Xiquan Li, Wenxi Chen, <b>Ziyang Ma</b>, Xuenan Xu, Yuzhe Liang, Zhisheng Zheng, Qiuqiang Kong, Xie Chen.</i><br>
<b><a href="https://arxiv.org/abs/2410.09472">DRCap: Decoding CLAP Latents with Retrieval-Augmented Generation for Zero-shot Audio Captioning.</a></b><br>
<font color=red>Oral</font> in <i>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2025.</i><br>
</p></li>
<li><p>
<i>Guanrou Yang, <b>Ziyang Ma</b>, Zhifu Gao, Shiliang Zhang, Xie Chen.</i><br>
<b><a href="https://arxiv.org/abs/2411.06437">CTC-Assisted LLM-Based Contextual ASR.</a></b><br>
in <i>IEEE Spoken Language Technology Workshop (SLT), 2024.</i><br>
</p></li>
<li><p>
<i>Guanrou Yang, <b>Ziyang Ma</b>, Fan Yu, Zhifu Gao, Shiliang Zhang, Xie Chen.</i><br>
<b><a href="https://arxiv.org/abs/2406.05839">MaLa-ASR: Multimedia-Assisted LLM-Based ASR.</a></b><br>
<font color=red>Oral</font> in <i>INTERSPEECH, 2024.</i><br>
</p></li>
<li><p>
<i>35 authors including Ziyang Ma.</i><br>
<b><a href="https://arxiv.org/abs/2402.16153">ChatMusician: Understanding and Generating Music Intrinsically with LLM.</a></b><br>
in <i>the annual meeting of the Association for Computational Linguistic (ACL), Findings, 2024.</i><br>
</p></li>
<li><p>
<i><b>Ziyang Ma</b>, Guanrou Yang, Yifan Yang, Zhifu Gao, Jiaming Wang, Zhihao Du, Fan Yu, Qian Chen, Siqi Zheng, Shiliang Zhang, Xie Chen.</i><br>
<b><a href="https://arxiv.org/abs/2402.08846">An Embarrassingly Simple Approach for LLM with Strong ASR Capacity.</a></b><br>
in <i>arXiv, 2024.</i><br>
</p></li>
<li><p>
<i>Zhisheng Zheng, Puyuan Peng, <b>Ziyang Ma</b>, Xie Chen, Eunsol Choi, David Harwath.</i><br>
<b><a href="https://arxiv.org/abs/2402.01591">BAT: Learning to Reason about Spatial Sounds with Large Language Models.</a></b><br>
in <i>International Conference on Machine Learning (ICML), 2024.</i><br>
</p></li>
<li><p>
<i>15 authors including Ziyang Ma.</i><br>
<b><a href="https://arxiv.org/abs/2310.04673">LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT.</a></b><br>
in <i>arXiv, 2023.</i><br>
</p></li>
</ul>

<p><b>Reasoning, Alignment and Post-training for Speech and Audio Processing</b></p>
<ul>
<li><p>
<i><b>Ziyang Ma</b>, Xiquan Li, Yakun Song, Wenxi Chen, Chenpeng Du, Jian Wu, Yuanzhe Chen, Zhuo Chen, Yuping Wang, Yuxuan Wang, Xie Chen.</i><br>
<b><a href="https://arxiv.org/abs/2505.19294">Towards Reliable Large Audio Language Model.</a></b><br>
in <i>the annual meeting of the Association for Computational Linguistic (ACL), Findings, 2025.</i><br>
</p></li>
<li><p>
<i>32 authors including Ziyang Ma.</i><br>
<b><a href="https://arxiv.org/abs/2505.16211">AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models.</a></b><br>
in <i>arXiv, 2025.</i><br>
</p></li>
<li><p>
<i><b>Ziyang Ma</b>, Yinghao Ma, Yanqiao Zhu, Chen Yang, Yi-Wen Chao, Ruiyang Xu and Others.</i><br>
<b><a href="https://arxiv.org/abs/2505.13032">MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix.</a></b><br>
in <i>arXiv, 2025.</i><br>
</p></li>
<li><p>
<i><b>Ziyang Ma</b>, Zhuo Chen, Yuping Wang, Eng Siong Chng, Xie Chen.</i><br>
<b><a href="https://arxiv.org/abs/2501.07246">Audio-CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language Model.</a></b><br>
in <i>arXiv, 2025.</i><br>
</p></li>
</ul>

<p><b>Generation, Interaction, and Dialog for Speech and Audio Processing</b></p>
<ul>
<li><p>
<i>Xinsheng Wang, Mingqi Jiang, <b>Ziyang Ma</b>, Ziyu Zhang, Songxiang Liu, Linqin Li and Others.</i><br>
<b><a href="https://arxiv.org/abs/2503.01710">Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens.</a></b><br>
in <i>arXiv, 2025.</i><br>
</p></li>
<li><p>
<i>57 authors including Ziyang Ma.</i><br>
<b><a href="https://arxiv.org/abs/2503.08638">YuE: Scaling Open Foundation Models for Long-Form Music Generation.</a></b><br>
in <i>arXiv, 2025.</i><br>
</p></li>
<li><p>
<i>Wenxi Chen, <b>Ziyang Ma</b>, Ruiqi Yan, Yuzhe Liang, Xiquan Li, Ruiyang Xu, Zhikang Niu, Yanqiao Zhu, Yifan Yang, Zhanxun Liu, Kai Yu, Yuxuan Hu, Jinyu Li, Yan Lu, Shujie Liu, Xie Chen.</i><br>
<b><a href="https://arxiv.org/abs/2412.15649">SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training.</a></b><br>
in <i>the annual meeting of the Association for Computational Linguistic (ACL), Findings, 2025.</i><br>
</p></li>
<li><p>
<i>Yushen Chen, Zhikang Niu, <b>Ziyang Ma</b>, Keqi Deng, Chunhui Wang, Jian Zhao, Kai Yu, Xie Chen.</i><br>
<b><a href="https://arxiv.org/abs/2410.06885">F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching.</a></b><br>
in <i>the annual meeting of the Association for Computational Linguistic (ACL), 2025.</i><br>
</p></li>
<li><p>
<i><b>Ziyang Ma</b>, Yakun Song, Chenpeng Du, Jian Cong, Zhuo Chen, Yuping Wang, Yuxuan Wang, Xie Chen.</i><br>
<b><a href="https://arxiv.org/abs/2408.02622">Language Model Can Listen While Speaking.</a></b><br>
<font color=red>Oral</font> in <i>the Annual AAAI Conference on Artificial Intelligence (AAAI), 2025.</i><br>
</p></li>
<li><p>
<i>12 authors including Ziyang Ma.</i><br>
<b><a href="https://arxiv.org/abs/2407.05407">CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens.</a></b><br>
in <i>arXiv, 2024.</i><br>
</p></li>
<li><p>
<i>Yiwei Guo, Chenpeng Du, <b>Ziyang Ma</b>, Xie Chen, Kai Yu.</i><br>
<b><a href="https://arxiv.org/abs/2309.05027">Voiceflow: Efficient text-to-speech with rectified flow matching.</a></b><br>
<font color=red>Oral</font> in <i>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024.</i><br>
</p></li>
</ul>

<p><b>Synthetic Data for Speech and Audio Processing</b></p>
<ul>
<li><p>
<i>Guanrou Yang, Fan Yu, <b>Ziyang Ma</b>, Zhihao Du, Zhifu Gao, Shiliang Zhang, Xie Chen.</i><br>
<b><a href="https://arxiv.org/abs/2410.16726">Enhancing Low-Resource ASR through Versatile TTS: Bridging the Data Gap.</a></b><br>
<font color=red>Oral</font> in <i>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2025.</i><br>
</p></li>
<li><p>
<i><b>Ziyang Ma</b>, Wen Wu, Zhisheng Zheng, Yiwei Guo, Qian Chen, Shiliang Zhang, Xie Chen.</i><br>
<b><a href="https://arxiv.org/abs/2309.10294">Leveraging Speech PTM, Text LLM, and Emotional TTS for Speech Emotion Recognition.</a></b><br>
<font color=red>Oral</font> in <i>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024.</i><br>
</p></li>
</ul>

<!-- <p>Note: * indicates the corresponding author.</p> -->
<!-- <p><a href="https://scholar.google.com/citations?user=4RZnXGMAAAAJ&hl=zh-CN&oi=ao">Full list of publications in Google Scholar</a>.</p> -->

<h3>Experiences</h3>
<p>Research Intern, SEED Speech Team, <a href="https://www.bytedance.com/en/">ByteDance</a>, 2024.05-2025.05</p>
<ul>
<li><p>Investigate full-duplex modeling for speech interaction and dialog system.</p>
</li>
<li><p>Led by <a href="https://scholar.google.com/citations?hl=en&user=3RaOfJkAAAAJ">Yuxuan Wang</a> and supervised by <a href="https://scholar.google.com/citations?user=pT8paUkAAAAJ&hl=en">Zhuo Chen</a>.</p>
</li>
</ul> 
<p>Research Intern, Tongyi Speech Lab, <a href="https://damo.alibaba.com/">Alibaba DAMO Academy</a>, 2023.06-2024.02</p>
<ul>
<li><p>Investigate emotional speech dialogue system with large language model.</p>
</li>
<li><p>Supervised by <a href="https://scholar.google.com/citations?user=BcWMSE4AAAAJ&hl=en">Shiliang Zhang</a> and working closely with <a href="https://scholar.google.com/citations?user=x6HfdMAAAAAJ&hl=en&oi=ao">Zhifu Gao</a> and <a href="https://scholar.google.com/citations?hl=en&user=8eosmSQAAAAJ">Qian Chen</a>.</p>
</li>
</ul> 
<p>Research Intern, NLC Group, <a href="https://www.msra.cn/">Microsoft Research Asia(MSRA)</a>, 2022.02-2022.08</p>
<ul>
<li><p>Investigate joint pre-training of speech and text to help improve the accuracy of ASR and other downstream tasks.</p>
</li>
<li><p>Led by <a href="http://gitnlp.org/">Furu Wei</a>, supervised by <a href="https://scholar.google.com/citations?user=6mNya-wAAAAJ&hl=en">Shujie Liu</a>, and working closely with <a href="https://sites.google.com/view/wuyu/home">Yu Wu</a> and <a href="https://long-zhou.github.io/">Long Zhou</a>.</p>
</li>
</ul> 
<p>Research Intern, Video Group, <a href="https://research.megvii.com/">MEGVII Research</a>, 2021.04-2021.06</p>
<ul>
<li><p>Investigate re-identification of vehicle with Transformer architecture.</p>
</li>
<li><p>Supervised by <a href="https://scholar.google.com/citations?user=bvPDWO4AAAAJ&hl=zh-CN&oi=sra">Chi Zhang</a>.</p>
</li>
</ul>
<p>Research Assistant, <a href="https://ilearn.qd.sdu.edu.cn/">InteLligent media research center (iLearn)</a>, Shandong University, 2020.09-2021.09</p>
<ul>
<li><p>Investigate temporal moment localization in untrimmed videos.</p>
</li>
<li><p>Supervised by <a href="https://xuemengsong.github.io/">Xuemeng Song</a> and <a href="https://liqiangnie.github.io/">Liqiang Nie</a>.</p>
</li>
</ul>

<h3>Academic Service</h3>
<p><b>Organizing Committee / Chair</b></p>
<ul>
<li><p>Multimodal Emotion Recognition Challenge (MER25) @ACM Multimedia MRAC25 Workshop</p>
</li>
<li><p>Speech Processing in LLM Era @ISCSLP 2024 Special Session</p>
</li>
<li><p>Multimodal Emotion Recognition Challenge (MER24) @ACM Multimedia MRAC24 Workshop</p>
</li>
</ul>
<p><b>Conference Reviewer / TPC Member</b></p>
<ul>
<li><p>Conference on Neural Information Processing Systems (NeurIPS) 2025</p>
</li>    
<li><p>ISCA Interspeech 2025</p>
</li>    
<li><p>International Conference on Learning Representations (ICLR) 2025</p>
</li>
<li><p>IEEE International Conference on Acoustics, Speech and Signal Processing (IEEE ICASSP) 2023, 2024, 2025</p>
</li>
<li><p>IEEE Spoken Language Technology Workshop (IEEE SLT) 2024</p>
</li>
<li><p>ACL Rolling Review (ACL ARR) 2024, 2025</p>
</li>
<li><p>AAAI Conference on Artificial Intelligence 2022</p>
</li>
<li><p>ACM International Conference on Multimedia (ACM MM) 2022</p>
</li>
</ul>
<p><b>Journal Reviewer</b></p>
<ul>
<li><p>IEEE Transactions on Audio, Speech and Language Processing (IEEE TASLP)</p>
</li>
<li><p>IEEE Signal Processing Letters (IEEE SPL)</p>
</li>
<li><p>IEEE Transactions on Multimedia (IEEE TMM)</p>
</li>
<li><p>IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCAVT)</p>
</li>
</ul>
<!-- <p><a href="https://publons.com/researcher/3034188/xiuze-zhou/">More details in Publons</a></p> -->

<h2>Open-Source Projects</h2>
<h3>Projects</h3>
<p><b>SLAM-LLM</b>[<a href="https://github.com/ddlBoJack/SLAM-LLM">GitHub</a>]</p>
<ul>
<li><p>SLAM-LLM is a deep learning toolkit that allows researchers and developers to train custom multimodal large language model (MLLM), focusing on Speech, Language, Audio, Music processing.</p>
</li>
</ul>
<p><b>FunAudioLLM</b>[<a href="https://github.com/FunAudioLLM">GitHub</a>][<a href="https://fun-audio-llm.github.io/pdf/FunAudioLLM.pdf">Techinical Report</a>][<a href="https://huggingface.co/FunAudioLLM">HuggingFace</a>][<a href="https://fun-audio-llm.github.io/">Demo</a>]</p>
<ul>
<li><p><b>SenseVoice</b> is a speech foundation model with multiple speech understanding capabilities.[<a href="https://github.com/FunAudioLLM/SenseVoice">GitHub</a>][<a href="https://www.modelscope.cn/models/iic/SenseVoiceSmall">ModelScope</a>]</p>
</li>
<li><p><b>CosyVoice</b> is a multi-lingual large voice generation model.[<a href="https://github.com/FunAudioLLM/CosyVoice">GitHub</a>][<a href="https://www.modelscope.cn/studios/iic/CosyVoice-300M">ModelScope</a>]</p>
</li>
</ul>
<p><b>emotion2vec series</b>[<a href="https://github.com/ddlBoJack/emotion2vec">GitHub</a>][<a href="https://arxiv.org/abs/2312.15185">emotion2vec(ACL2024)</a>][<a href="https://huggingface.co/emotion2vec">HuggingFace</a>][<a href="https://modelscope.cn/models?name=emotion2vec">ModelScope</a>]</p>
<ul>
<li><p><b>emotion2vec</b> is the first universal speech emotion representation model.</p>
</li>
<li><p><b>emotion2vec+</b> is a series of foundational models for speech emotion recognition (SER).</p>
</li>
</ul>
<p><b>MAP-Neo series</b>[<a href="https://github.com/multimodal-art-projection/MAP-NEO">GitHub</a>][<a href="https://arxiv.org/abs/2405.19327">Techinical Report</a>][<a href="https://huggingface.co/collections/m-a-p/neo-models-66395a5c9662bb58d5d70f04">HuggingFace</a>]</p>
<ul>
<li><p><b>MAP-Neo</b> is a series of fully open-sourced large language models.</p>
</li>
<li><p><b>Matrix</b> is the pretraining data and data processing pipeline for MAP-Neo.[<a href="https://huggingface.co/datasets/m-a-p/Matrix">Dataset</a>]</p>
</li>
</ul>

<h2>Accomplishments</h2>
<h3>Awards</h3>
<ul>
<li><p>SPS Travel Grant, IEEE, 2024.02</p>
</li>
<li><p>Best Presentation Award in Student Forum, the 18th National Conference on Man-Machine Speech Communication (NCMMSC), 2023.12</p>
</li>
<li><p>Interspeech Best Student Paper Shortlist, ISCA, 2023.08</p>
</li>
<li><p>Excellent Graduate, Department of Education, Shandong Province, China, 2022.06</p>
</li>
<li><p>"Intelligent Pedestal" Scholarship, Huawei, 2021.12</p>
</li>
<li><p>SIGMM Student Travel Grant, ACM, 2021.11</p>
</li>
<li><p>National Scholarship, Ministry of Education, China, 2021.10</p>
</li>
</ul>

<h3>Competitions</h3>
<ul>
<li><p>3rd in <a href="https://dcase.community/challenge2024/task-automated-audio-captioning">DCASE 2024 Challenge Task6: Automated Audio Captioning</a>, IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events, 2024.06.</p></li>
<li><p>1st in <a href="https://www.odyssey2024.org/emotion-recognition-challenge">Odyssey 2024 Emotion Recognition Challenge Task1: Categorical Emotion Recognition</a>, Odyssey 2024 The Speaker and Language Recognition Workshop, 2024.03. </p></li>
<li><p>3rd in <a href="https://dcase.community/challenge2023/task-sound-event-detection-with-soft-labels">DCASE 2023 Challenge Task4b: Sound Event Detection with Soft Labels</a>, IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events, 2023.06.</p></li>
<li><p>3rd in <a href="http://www.aiwin.org.cn/competitions/69">Avatar Track of AIWIN</a>, the 5th World Artificial Intelligence Conference(WAIC2022), Shanghai, China, 2022.09.[<a href="https://mp.weixin.qq.com/s/3wuZG4I4PbXW9WmAfuVFwQ">Report</a>][<a href="https://www.bilibili.com/video/BV1iV4y137RC">Invited Talk</a>]</p></li>
<li><p>Finalist(Top 284 in 26112 teams) in <a href="https://www.comap.com/undergraduate/contests/mcm/contests/2021/results/">Mathematical  Contest in Modeling (MCM)</a>, Consortium for Mathematics and Its Application, America, 2021.02</p></li>
<li><p>First Prize(Top 293 in 45689 teams) in <a href="http://www.mcm.edu.cn/">Contemporary Undergraduate Mathematical Contest in Modeling (CUMCM)</a>, China Society for Industrial and Applied Mathematics, China, 2020.09</p></li>
</ul>

<h3>Activities</h3>
<ul>
<li><p>Invited Talk: Towards Interactive Speech Language Model, Nvidia, 2024.10</p></li>
<li><p>Invited Talk: Towards Interactive Speech Language Model, The Hong Kong University of Science and Technology(HKUST), 2024.8</p></li>
<li><p>Invited Talk: Speech & Audio Understanding Based on SSL and LLM, Nvidia, 2024.6</p></li>
<li><p>Invited Talk: INTERSPEECH 2023 Pre-presentation, SpeechHome, 2023.07</p></li>
<li><p>Invited Talk: Towards More Realistic, Powerful, and Accurate Speech-based Self-Supervised Learning </a>, The Renmin University of China(RUC), 2023.5</p></li>
<li><p>PhD Debate Towards AIGC, AI TIME, 2023.1</p></li>
<li><p>[<a href="https://www.bilibili.com/video/BV1iV4y137RC">Invited Talk</a>]: How to conduct audio-driven talking head? An introduction and solution sharing, Datawhale, 2022.11</p></li>
<li><p>Member of <a href="https://datawhale.club/">Datawhale</a>, 2022.09-Now</p></li>
<li><p>Teaching Assistant, Computer Science and Technology, Shandong University, 2021.03-2021.06</p></li>
<li><p>Member of Elite Class, Computer Science and Technology, Shandong University, 2020.09-2022.06</p></li>
</ul>

<!-- <p><br><a href="cv/cv.pdf">A brief cv</a>.</p> -->
<center>
<a href="https://clustrmaps.com/site/1bonu" title="Visit tracker"><img src="https://clustrmaps.com/map_v2.png?cl=ffffff&w=300&t=tt&d=6yrMMIqIUJ1GT9nydxBO8cdtx5nw8iVTjA_d5hAcBcY&co=2d78ad&ct=ffffff" /></a>
</center>
</td>
</tr>
</table>
</body>
</html>
