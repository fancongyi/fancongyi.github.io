
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" href="favicon.ico" />
<link rel="bookmark" href="favicon.ico" type="image/x-icon"　/>
<title>Congyi Fan (范聪毅)</title>
<style>
  /* 把所有 <b> 内的链接都设成橙色 */
  b a {
    color: #224b8d;       /* 你喜欢的橙色 */
    text-decoration: underline;  /* 保留下划线 */
  }
  /* 你还可以定义 hover、visited 等状态 */
  b a:hover {
    color: #3069C6;       /* 鼠标悬停时稍微亮一点 */
  }
  b a:visited {
    color: #843F9F;       /* 访问过后换成深紫 */
  }
</style>
</head>

<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="https://scholar.google.com/citations?user=bBNwgX8AAAAJ">Scholar</a></div>
<div class="menu-item"><a href="https://github.com/fancongyi">GitHub</a></div>
<!-- <div class="menu-item"><a href="https://www.linkedin.com/in/ziyang-ma">Linkedin</a></div> -->
<!-- <div class="menu-item"><a href="https://blog.csdn.net/weixin_45019478">Blog</a></div> -->
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Congyi Fan (范聪毅)</h1>
</div>
<table class="imgtable">
  <tr>
    <td>
      <a href="index.html">
        <img src="pic/avatar.jpeg" alt="Congyi Fan's Avatar" width="120px" height="160px" />
      </a>&nbsp;
    </td>
    <td align="left">
      <p>
        Undergraduate Student,<br />
        Harbin Engineering University,<br />
        Harbin, Heilongjiang Province, China.<br />
        <a href="mailto:fancongyi@hrbeu.edu.cn">fancongyi@hrbeu.edu.cn</a>
      </p>
    </td>
  </tr>
</table>



<h2>Biography</h2>
<p>Hi👋 nice to meet you!</p>
<p>Currently I am within the Joint Ph.D. Programme of Shanghai Jiao Tong University (SJTU) and Nanyang Technological University (NTU), co-supervised by Prof. <a href="https://chenxie95.github.io/">Xie Chen</a> from SJTU and Prof. <a href="https://aseschng.github.io/intro1.html">Chng Eng Siong</a> from NTU. I am also a member in <a href="https://x-lance.sjtu.edu.cn/">Cross Media (<b>X-</b>) <b>Lan</b>guage Intelligen<b>ce</b> Lab (X-LANCE)</a>, working closely with Prof. <a href="https://x-lance.sjtu.edu.cn/members/kai_yu">Kai Yu</a>. As the first Ph.D. supervised by Prof. Chen, I will try my best in the next five exciting years! 💪</p>
<p>I was a research assistant at <a href="https://ilearn.qd.sdu.edu.cn/"><b>I</b>nte<b>L</b>ligent m<b>e</b>di<b>a</b> <b>r</b>esearch ce<b>n</b>ter (iLearn)</a>, working closely with Prof. <a href="https://xuemengsong.github.io/">Xuemeng Song</a> and <a href="https://liqiangnie.github.io/">Liqiang Nie</a> during my undergraduate years.<p>
<p>My research usually follows the <a href="https://wikipedia.org/wiki/KISS_principle">KISS</a> philosophy. My recent work focuses on speech, language, audio and music processing with Self-Supervised Learning (SSL) and Large Language Model (LLM). If you are also interested, please feel free to contact me.</p>

<h3>Education</h3>
<ul>
<!-- <li><p>Ph.D., Computer Science and Engineering, Shanghai Jiao Tong University, 2022.09-Now</p>
</li>
<li><p>Ph.D., Computer Science and Engineering, Nanyang Technological University, 2022.09-Now</p>
</li> -->
<li><p>B.E., Information Security, Harbin Engineering University, 2022.09-Now</p>
</li>
</ul>

<h3>Interests</h3>
<ul>
<li><p>Generative Models</p>
</li>
<li><p>3D Vision</p>
</li>
<li><p>Multimodal</p>
<!-- </li>
<li><p>Multimodal Reasoning</p> -->
</li>
</ul>

<style>
/* NEWS容器样式 */
.news-container {
    max-height: 400px;       /* 设置最大高度 */
    overflow-y: auto;        /* 垂直方向自动显示滚动条 */
    padding-right: 15px;     /* 为滚动条预留空间 */
    margin: 20px 0;          /* 添加外边距 */
    border: 1px solid #eee;  /* 可选边框 */
    border-radius: 8px;      /* 圆角 */
}

/* 自定义滚动条样式 */
.news-container::-webkit-scrollbar {
    width: 6px;              /* 滚动条宽度 */
}
.news-container::-webkit-scrollbar-track {
    background: #f1f1f1;     /* 轨道颜色 */
    border-radius: 3px;
}
.news-container::-webkit-scrollbar-thumb {
    background: #888;        /* 滑块颜色 */
    border-radius: 3px;
}
.news-container::-webkit-scrollbar-thumb:hover {
    background: #555;        /* 鼠标悬停颜色 */
}

/* 调整原有样式 */
.news-container h3 {
    margin: 15px 0;
    position: sticky;        /* 固定标题 */
    top: 0;
    background: white;
    z-index: 1;
    padding: 10px 0;
}
.news-container ul {
    margin: 0;
    padding: 0 0 15px 25px;  /* 调整列表内边距 */
}
</style>

<!-- HTML结构 -->
<div class="news-container">
<h3>NEWS</h3>
<!-- <img src="pic/new_gif.gif" alt="alt text"/> -->
<ul>
<li><p>[2025.5] <img src="pic/new_gif.gif" alt="alt text"/> Check out our MMAR, a new benchmark designed to evaluate the deep reasoning capabilities of Audio-Language Models (ALMs).[<a href="https://arxiv.org/abs/2505.13032">arXiv</a>][<a href="https://www.youtube.com/watch?v=Dab13opIGqU">Demo</a>][<a href="https://github.com/ddlBoJack/MMAR">GitHub</a>][<a href="https://huggingface.co/datasets/BoJack/MMAR">Benchmark</a>] </p></li>
<li><p>[2025.5] 1 papers was accpeted by ICCV 2025. </p></li>
<li><p>[2025.4] Selected as a Pioneer in Innovation and Entrepreneurship (Top 0.03%).</p></li>
<li><p>[2025.4] Received Outstanding Final Report Award for the National Undergraduate Training Program for Innovation and Entrepreneurship.</p></li>
<li><p>[2025.3] New paper released! Align Your Rhythm: Generating Highly Aligned Dance Poses with Gating-Enhanced Rhythm-Aware Feature Representation.</p></li>
<li><p>[2024.12] Awarded First Prize at the 9th National Youth Artificial Intelligence Innovation and Entrepreneurship Conference.</p></li>
<li><p>[2024.11] FastDrag was successfully selected as an Outstanding Achievement in the First Academic Annual Conference of Harbin Engineering University.</p></li>
<li><p>[2024.9] I have received a letter of appreciation from the Heilongjiang Provincial Department of Education.</p></li>
<li><p>[2024.07] As the project leader, successfully led the application for the 2024 National Undergraduate Training Program for Innovation and Entrepreneurship (Top 4.16%).</p></li>
<li><p>[2023.9] Participated in the 2023 Forum on Technological Innovation for the Maritime Power Strategy.</p></li>
<li><p>[2023.8] Participated in the 18th National Conference on Computer-Supported Cooperative Work and Social Computing.</p></li>
<!-- <li><p> </p></li>
<li><p> </p></li>
<li><p> </p></li>
<li><p> </p></li>
<li><p> </p></li>
<li><p> </p></li> -->
</ul>
</div>


<h2>Research</h2>
<h3>Selected Publications</h3>
<p>Thanks to all the collaborators for their great work!</p>
<p>Check out <a href="https://scholar.google.com/citations?user=bBNwgX8AAAAJ">Google Scholar</a> for more information.</p>

<!-- <p><b>Speech, Language, Audio, Music Processing with SSL</b></p> -->
<ul>
<!-- <li><p>
<i>Haina Zhu, Yizhi Zhou, Hangting Chen, Jianwei Yu, <b>Ziyang Ma</b>, Rongzhi Gu, Yi Luo, Wei Tan, Xie Chen
.</i><br>
<b><a href="https://arxiv.org/abs/2501.01108">MuQ: Self-Supervised Music Representation Learning with Mel Residual Vector Quantization.</a></b><br>
in <i>arXiv, 2025.</i><br>
</p></li> -->
<li><p>
<i><span style="color:#224b8d"><b>Congyi Fan</b></span>, Jian Guan, Xuanjia Zhao, Dongli Xu, Youtian Lin, Tong Ye, Haiwei Pan, Pengming Feng
.</i><br>
<b><a href="https://arxiv.org/abs/2503.17340">Align Your Rhythm: Generating Highly Aligned Dance Poses with Gating-Enhanced Rhythm-Aware Feature Representation.</a></b><br>
in <i>ICCV 2025.</i><br>
</p></li>
<li><p>
<i>Xuanjia Zhao, Jian Guan, <span style="color:#224b8d"><b>Congyi Fan</b></span>, Dongli Xu, Youtian Lin, Haiwei Pan, Pengming Feng
.</i><br>
<b><a href="https://arxiv.org/pdf/2405.15769">FastDrag: Manipulate Anything in One Step.</a></b><br>
in <i>NeurIPS 2024.</i><br>
</p></li>
<li><p>
<i><span style="color:#224b8d"><b>Congyi Fan*</b></span>, Shitong Fan*, Feiyang Xiao, Wenbo Wang, Xinyi Che, Qiaoxi Zhu, Jian Guan
.</i><br>
<b><a href="https://dcase.community/documents/challenge2025/technical_reports/DCASE2025_Guan_77_t3.pdf">GISP@HEU's Submission to the DCASE 2025 Challenge: Stereo SELD Task.</a></b><br>
in <i>Dcase 2025 Technical Report.</i><br>
</p></li>
</ul>

<!-- <p>Note: * indicates the corresponding author.</p> -->
<!-- <p><a href="https://scholar.google.com/citations?user=4RZnXGMAAAAJ&hl=zh-CN&oi=ao">Full list of publications in Google Scholar</a>.</p> -->

<h3>Experiences</h3>
<p>Research Intern, SEED Speech Team, <a href="https://www.bytedance.com/en/">ByteDance</a>, 2024.05-2025.05</p>
<ul>
<li><p>Investigate full-duplex modeling for speech interaction and dialog system.</p>
</li>
<li><p>Led by <a href="https://scholar.google.com/citations?hl=en&user=3RaOfJkAAAAJ">Yuxuan Wang</a> and supervised by <a href="https://scholar.google.com/citations?user=pT8paUkAAAAJ&hl=en">Zhuo Chen</a>.</p>
</li>
</ul> 
<p>Research Intern, Tongyi Speech Lab, <a href="https://damo.alibaba.com/">Alibaba DAMO Academy</a>, 2023.06-2024.02</p>
<ul>
<li><p>Investigate emotional speech dialogue system with large language model.</p>
</li>
<li><p>Supervised by <a href="https://scholar.google.com/citations?user=BcWMSE4AAAAJ&hl=en">Shiliang Zhang</a> and working closely with <a href="https://scholar.google.com/citations?user=x6HfdMAAAAAJ&hl=en&oi=ao">Zhifu Gao</a> and <a href="https://scholar.google.com/citations?hl=en&user=8eosmSQAAAAJ">Qian Chen</a>.</p>
</li>
</ul> 
<p>Research Intern, NLC Group, <a href="https://www.msra.cn/">Microsoft Research Asia(MSRA)</a>, 2022.02-2022.08</p>
<ul>
<li><p>Investigate joint pre-training of speech and text to help improve the accuracy of ASR and other downstream tasks.</p>
</li>
<li><p>Led by <a href="http://gitnlp.org/">Furu Wei</a>, supervised by <a href="https://scholar.google.com/citations?user=6mNya-wAAAAJ&hl=en">Shujie Liu</a>, and working closely with <a href="https://sites.google.com/view/wuyu/home">Yu Wu</a> and <a href="https://long-zhou.github.io/">Long Zhou</a>.</p>
</li>
</ul> 
<p>Research Intern, Video Group, <a href="https://research.megvii.com/">MEGVII Research</a>, 2021.04-2021.06</p>
<ul>
<li><p>Investigate re-identification of vehicle with Transformer architecture.</p>
</li>
<li><p>Supervised by <a href="https://scholar.google.com/citations?user=bvPDWO4AAAAJ&hl=zh-CN&oi=sra">Chi Zhang</a>.</p>
</li>
</ul>
<p>Research Assistant, <a href="https://ilearn.qd.sdu.edu.cn/">InteLligent media research center (iLearn)</a>, Shandong University, 2020.09-2021.09</p>
<ul>
<li><p>Investigate temporal moment localization in untrimmed videos.</p>
</li>
<li><p>Supervised by <a href="https://xuemengsong.github.io/">Xuemeng Song</a> and <a href="https://liqiangnie.github.io/">Liqiang Nie</a>.</p>
</li>
</ul>

<h3>Academic Service</h3>
<p><b>Organizing Committee / Chair</b></p>
<ul>
<li><p>Multimodal Emotion Recognition Challenge (MER25) @ACM Multimedia MRAC25 Workshop</p>
</li>
<li><p>Speech Processing in LLM Era @ISCSLP 2024 Special Session</p>
</li>
<li><p>Multimodal Emotion Recognition Challenge (MER24) @ACM Multimedia MRAC24 Workshop</p>
</li>
</ul>
<p><b>Conference Reviewer / TPC Member</b></p>
<ul>
<li><p>Conference on Neural Information Processing Systems (NeurIPS) 2025</p>
</li>    
<li><p>ISCA Interspeech 2025</p>
</li>    
<li><p>International Conference on Learning Representations (ICLR) 2025</p>
</li>
<li><p>IEEE International Conference on Acoustics, Speech and Signal Processing (IEEE ICASSP) 2023, 2024, 2025</p>
</li>
<li><p>IEEE Spoken Language Technology Workshop (IEEE SLT) 2024</p>
</li>
<li><p>ACL Rolling Review (ACL ARR) 2024, 2025</p>
</li>
<li><p>AAAI Conference on Artificial Intelligence 2022</p>
</li>
<li><p>ACM International Conference on Multimedia (ACM MM) 2022</p>
</li>
</ul>
<p><b>Journal Reviewer</b></p>
<ul>
<li><p>IEEE Transactions on Audio, Speech and Language Processing (IEEE TASLP)</p>
</li>
<li><p>IEEE Signal Processing Letters (IEEE SPL)</p>
</li>
<li><p>IEEE Transactions on Multimedia (IEEE TMM)</p>
</li>
<li><p>IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCAVT)</p>
</li>
</ul>
<!-- <p><a href="https://publons.com/researcher/3034188/xiuze-zhou/">More details in Publons</a></p> -->

<h2>Open-Source Projects</h2>
<h3>Projects</h3>
<p><b>SLAM-LLM</b>[<a href="https://github.com/ddlBoJack/SLAM-LLM">GitHub</a>]</p>
<ul>
<li><p>SLAM-LLM is a deep learning toolkit that allows researchers and developers to train custom multimodal large language model (MLLM), focusing on Speech, Language, Audio, Music processing.</p>
</li>
</ul>
<p><b>FunAudioLLM</b>[<a href="https://github.com/FunAudioLLM">GitHub</a>][<a href="https://fun-audio-llm.github.io/pdf/FunAudioLLM.pdf">Techinical Report</a>][<a href="https://huggingface.co/FunAudioLLM">HuggingFace</a>][<a href="https://fun-audio-llm.github.io/">Demo</a>]</p>
<ul>
<li><p><b>SenseVoice</b> is a speech foundation model with multiple speech understanding capabilities.[<a href="https://github.com/FunAudioLLM/SenseVoice">GitHub</a>][<a href="https://www.modelscope.cn/models/iic/SenseVoiceSmall">ModelScope</a>]</p>
</li>
<li><p><b>CosyVoice</b> is a multi-lingual large voice generation model.[<a href="https://github.com/FunAudioLLM/CosyVoice">GitHub</a>][<a href="https://www.modelscope.cn/studios/iic/CosyVoice-300M">ModelScope</a>]</p>
</li>
</ul>
<p><b>emotion2vec series</b>[<a href="https://github.com/ddlBoJack/emotion2vec">GitHub</a>][<a href="https://arxiv.org/abs/2312.15185">emotion2vec(ACL2024)</a>][<a href="https://huggingface.co/emotion2vec">HuggingFace</a>][<a href="https://modelscope.cn/models?name=emotion2vec">ModelScope</a>]</p>
<ul>
<li><p><b>emotion2vec</b> is the first universal speech emotion representation model.</p>
</li>
<li><p><b>emotion2vec+</b> is a series of foundational models for speech emotion recognition (SER).</p>
</li>
</ul>
<p><b>MAP-Neo series</b>[<a href="https://github.com/multimodal-art-projection/MAP-NEO">GitHub</a>][<a href="https://arxiv.org/abs/2405.19327">Techinical Report</a>][<a href="https://huggingface.co/collections/m-a-p/neo-models-66395a5c9662bb58d5d70f04">HuggingFace</a>]</p>
<ul>
<li><p><b>MAP-Neo</b> is a series of fully open-sourced large language models.</p>
</li>
<li><p><b>Matrix</b> is the pretraining data and data processing pipeline for MAP-Neo.[<a href="https://huggingface.co/datasets/m-a-p/Matrix">Dataset</a>]</p>
</li>
</ul>

<h2>Accomplishments</h2>
<h3>Awards</h3>
<ul>
<li><p>SPS Travel Grant, IEEE, 2024.02</p>
</li>
<li><p>Best Presentation Award in Student Forum, the 18th National Conference on Man-Machine Speech Communication (NCMMSC), 2023.12</p>
</li>
<li><p>Interspeech Best Student Paper Shortlist, ISCA, 2023.08</p>
</li>
<li><p>Excellent Graduate, Department of Education, Shandong Province, China, 2022.06</p>
</li>
<li><p>"Intelligent Pedestal" Scholarship, Huawei, 2021.12</p>
</li>
<li><p>SIGMM Student Travel Grant, ACM, 2021.11</p>
</li>
<li><p>National Scholarship, Ministry of Education, China, 2021.10</p>
</li>
</ul>

<h3>Competitions</h3>
<ul>
<li><p>3rd in <a href="https://dcase.community/challenge2024/task-automated-audio-captioning">DCASE 2024 Challenge Task6: Automated Audio Captioning</a>, IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events, 2024.06.</p></li>
<li><p>1st in <a href="https://www.odyssey2024.org/emotion-recognition-challenge">Odyssey 2024 Emotion Recognition Challenge Task1: Categorical Emotion Recognition</a>, Odyssey 2024 The Speaker and Language Recognition Workshop, 2024.03. </p></li>
<li><p>3rd in <a href="https://dcase.community/challenge2023/task-sound-event-detection-with-soft-labels">DCASE 2023 Challenge Task4b: Sound Event Detection with Soft Labels</a>, IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events, 2023.06.</p></li>
<li><p>3rd in <a href="http://www.aiwin.org.cn/competitions/69">Avatar Track of AIWIN</a>, the 5th World Artificial Intelligence Conference(WAIC2022), Shanghai, China, 2022.09.[<a href="https://mp.weixin.qq.com/s/3wuZG4I4PbXW9WmAfuVFwQ">Report</a>][<a href="https://www.bilibili.com/video/BV1iV4y137RC">Invited Talk</a>]</p></li>
<li><p>Finalist(Top 284 in 26112 teams) in <a href="https://www.comap.com/undergraduate/contests/mcm/contests/2021/results/">Mathematical  Contest in Modeling (MCM)</a>, Consortium for Mathematics and Its Application, America, 2021.02</p></li>
<li><p>First Prize(Top 293 in 45689 teams) in <a href="http://www.mcm.edu.cn/">Contemporary Undergraduate Mathematical Contest in Modeling (CUMCM)</a>, China Society for Industrial and Applied Mathematics, China, 2020.09</p></li>
</ul>

<h3>Activities</h3>
<ul>
<li><p>Invited Talk: Towards Interactive Speech Language Model, Nvidia, 2024.10</p></li>
<li><p>Invited Talk: Towards Interactive Speech Language Model, The Hong Kong University of Science and Technology(HKUST), 2024.8</p></li>
<li><p>Invited Talk: Speech & Audio Understanding Based on SSL and LLM, Nvidia, 2024.6</p></li>
<li><p>Invited Talk: INTERSPEECH 2023 Pre-presentation, SpeechHome, 2023.07</p></li>
<li><p>Invited Talk: Towards More Realistic, Powerful, and Accurate Speech-based Self-Supervised Learning </a>, The Renmin University of China(RUC), 2023.5</p></li>
<li><p>PhD Debate Towards AIGC, AI TIME, 2023.1</p></li>
<li><p>[<a href="https://www.bilibili.com/video/BV1iV4y137RC">Invited Talk</a>]: How to conduct audio-driven talking head? An introduction and solution sharing, Datawhale, 2022.11</p></li>
<li><p>Member of <a href="https://datawhale.club/">Datawhale</a>, 2022.09-Now</p></li>
<li><p>Teaching Assistant, Computer Science and Technology, Shandong University, 2021.03-2021.06</p></li>
<li><p>Member of Elite Class, Computer Science and Technology, Shandong University, 2020.09-2022.06</p></li>
</ul>

<!-- <p><br><a href="cv/cv.pdf">A brief cv</a>.</p> -->
<center>
<a href="https://clustrmaps.com/site/1bonu" title="Visit tracker"><img src="https://clustrmaps.com/map_v2.png?cl=ffffff&w=300&t=tt&d=6yrMMIqIUJ1GT9nydxBO8cdtx5nw8iVTjA_d5hAcBcY&co=2d78ad&ct=ffffff" /></a>
</center>
</td>
</tr>
</table>
</body>
</html>
