<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="jemdoc.css" type="text/css" />
  <link rel="icon" href="favicon.ico" type="image/x-icon" />
  <title>Congyi Fan (èŒƒèªæ¯…)</title>

  <!--
    Clean, modern overrides for jemdoc.
    Goals: ç®€æ´ã€å¯¹é½ã€ç•™ç™½èˆ’æœã€ç§»åŠ¨ç«¯å¯è¯»ã€å·¦å³åˆ†æ æ›´æ¸…æ™°ã€‚
    ä¿æŒ XHTML 1.1 å…¼å®¹ï¼Œä¸å¼•å…¥å¤–éƒ¨åº“/å­—ä½“ã€‚
  -->
  <style type="text/css">
    /* ---------- CSS Variables (safe in XHTML) ---------- */
    :root {
      --bg: #ffffff;
      --card: #ffffff;
      --text: #111827; /* gray-900 */
      --muted: #6b7280; /* gray-500 */
      --border: #e5e7eb; /* gray-200 */
      --menu-bg: #f8fafc; /* gray-50 */
      --menu-text: #374151; /* gray-700 */
      --primary: #224b8d;
      --primary-hover: #3069C6;
      --highlight: #e8f0fe; /* subtle selection bg */
    }

    /* ---------- Base Typography ---------- */
    html { scroll-behavior: smooth; }
    body {
      margin: 0;
      background: var(--bg);
      color: var(--text);
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
        "Helvetica Neue", Arial, "Noto Sans CJK SC", "PingFang SC",
        "Microsoft YaHei", sans-serif;
      line-height: 1.7;
      font-size: 16px;
      -webkit-font-smoothing: antialiased;
      -moz-osx-font-smoothing: grayscale;
    }

    h1, h2, h3, h4, h5, h6 {
      color: var(--text);
      margin: 0.6em 0 0.4em 0;
      line-height: 1.25;
    }
    h1 { font-size: 2rem; }
    h2 { font-size: 1.5rem; border-bottom: 1px solid var(--border); padding-bottom: 6px; }
    h3 { font-size: 1.15rem; color: #0f172a; /* slate-900 */ }

    p { margin: 0.45em 0; }
    .muted { color: var(--muted); }

    a {
      color: var(--primary);
      text-decoration: none;
      border-bottom: 1px solid rgba(34,75,141,0.25);
    }
    a:hover { color: var(--primary-hover); border-bottom-color: rgba(48,105,198,0.45); }
    a:visited { color: var(--primary); }

    /* ---------- Layout (keep jemdoc table but make it feel modern) ---------- */
    #tlayout { margin: 0 auto; max-width: 1100px; border-collapse: collapse; width: 100%; }
    #layout-menu { width: 200px; background: var(--menu-bg); border-right: 1px solid var(--border); }
    #layout-content { background: var(--card); padding: 28px 32px; }

    /* Make the left menu sticky on desktop */
    #layout-menu > div { position: sticky; top: 16px; }

    /* Menu styles */
    .menu-category {
      font-weight: 700; color: var(--menu-text); text-transform: uppercase;
      letter-spacing: .06em; font-size: 0.8rem; padding: 18px 16px 8px 16px;
    }
    .menu-item { padding: 6px 16px; }
    .menu-item a {
      display: block; padding: 8px 10px; border-radius: 10px;
      border: 1px solid transparent; background: transparent;
    }
    .menu-item a:hover { background: #eef2ff; border-color: #e0e7ff; }
    .menu-item a.current { background: #e0ecff; border-color: #c7dbff; font-weight: 600; }

    /* Top title area */
    #toptitle h1 { margin-top: 0; font-size: 2.15rem; }

    /* Header block: avatar + info */
    .imgtable { width: 100%; border-spacing: 0; }
    .imgtable td { vertical-align: middle; padding: 0; }
    .avatar { border-radius: 12px; box-shadow: 0 3px 12px rgba(0,0,0,.12); width: 140px; height: 140px; display: block; }
    .intro { padding-left: 18px; }
    .intro p { margin: 2px 0; }

    /* Lists spacing (publications/news/etc.) */
    ul { margin: 0.4em 0 0.6em 1.2em; padding: 0; }
    li { margin: 0.35em 0; }
    li p { margin: 0.15em 0; }

    /* Section cards (optional soft grouping) */
    .section { margin: 18px 0 22px 0; }

    /* News scroller */
    .news-container {
      max-height: 360px; overflow-y: auto; padding: 8px 14px; margin: 14px 0;
      border: 1px solid var(--border); border-radius: 12px; background: #fbfbfd;
    }
    .news-container h3 { position: sticky; top: -8px; background: #fbfbfd; margin: 8px 0 6px 0; padding: 8px 0; z-index: 1; }
    .news-container ul { margin-left: 1em; padding-bottom: 8px; }

    /* Publications */
    .pubs { list-style: none; margin-left: 0; }
    .pubs li { padding: 12px 14px; border: 1px solid var(--border); border-radius: 12px; margin-bottom: 10px; background: var(--card); }
    .pub-title { font-weight: 700; }
    .authors i { font-style: normal; }
    .me { font-weight: 800; color: var(--primary); }
    .venue { font-style: italic; }
    .links a { margin-right: 10px; }

    /* Subtle footer note */
    .footnote { font-size: 0.92rem; color: var(--muted); }

    /* Mobile: stack columns */
    @media (max-width: 880px) {
      #tlayout, #tlayout tr, #tlayout td { display: block; width: 100%; }
      #layout-menu { width: auto; border-right: none; border-bottom: 1px solid var(--border); }
      #layout-menu > div { position: static; }
      #layout-content { padding: 20px 16px; }
      .avatar { width: 120px; height: 120px; }
      .intro { padding-left: 12px; }
    }

    /* Dark mode (auto) */
    @media (prefers-color-scheme: dark) {
      :root {
        --bg: #0b0f1a;
        --card: #0e1422;
        --text: #e5e7eb;
        --muted: #9aa3b2;
        --border: #1f2937;
        --menu-bg: #0e1422;
        --menu-text: #cbd5e1;
        --primary: #86b7ff;
        --primary-hover: #a5c7ff;
        --highlight: #172554;
      }
      .menu-item a.current { background: #0f1a2e; border-color: #1d2a44; }
      .menu-item a:hover { background: #0d1b2a; border-color: #1b2b44; }
      .news-container { background: #0d1320; }
      .news-container h3 { background: #0d1320; }
    }
  </style>
</head>

<body>
  <table summary="Table for page layout." id="tlayout">
    <tr valign="top">
      <td id="layout-menu">
        <div>
          <div class="menu-category">Menu</div>
          <div class="menu-item"><a href="index.html" class="current">Home</a></div>
          <div class="menu-item"><a href="https://scholar.google.com/citations?user=bBNwgX8AAAAJ">Scholar</a></div>
          <div class="menu-item"><a href="https://github.com/fancongyi">GitHub</a></div>
          <!-- <div class="menu-item"><a href="https://www.linkedin.com/in/ziyang-ma">LinkedIn</a></div> -->
          <!-- <div class="menu-item"><a href="https://blog.csdn.net/weixin_45019478">Blog</a></div> -->
        </div>
      </td>

      <td id="layout-content">
        <div id="toptitle">
          <h1>Congyi Fan (èŒƒèªæ¯…)</h1>
        </div>

        <table class="imgtable">
          <tr>
            <td>
              <a href="index.html">
                <img src="profile.png" alt="Congyi Fan" class="avatar" />
              </a>
            </td>
            <td class="intro" align="left">
              <p>Undergraduate Student<br />
                Harbin Engineering University, China<br />
                Harbin, Heilongjiang Province</p>
              <p><a href="mailto:fancongyi@hrbeu.edu.cn">fancongyi@hrbeu.edu.cn</a></p>
            </td>
          </tr>
        </table>

        <div class="section">
          <h2>Biography</h2>
          <p>Hi ğŸ‘‹ nice to meet you!</p>
          <p>Currently I am an undergraduate student majoring in Computer Science and Technology (Information Security) at Harbin Engineering University (HEU), serving as a research assistant to Associate Professor <a href="https://scholar.google.com/citations?user=wf60G1sAAAAJ">Jian Guan</a>.</p>
          <p>My research is jointly supervised by Associate Professor Jian Guan at HEU, Researcher <a href="https://scholar.google.com/citations?user=O51mMKgAAAAJ">Pengming Feng</a> from the State Key Laboratory of Space-Ground Integrated Information Technology (SGIIT), and Dr. <a href="https://scholar.google.com/citations?user=H56DRXwAAAAJ">Qiaoxi Zhu</a> from the University of Technology Sydney.</p>
          <p>My research is guided by the aspiration to advance scientific and technological progress. My recent work focuses on <strong>Generative Models, 3D Vision, and Multimodal</strong>. If you are also interested, please feel free to contact me.</p>
        </div>

        <div class="section">
          <h3>Education</h3>
          <ul>
            <li><p>B.E., Information Security, Harbin Engineering University, 2022.09â€“Now</p></li>
          </ul>
        </div>

        <div class="section">
          <h3>Interests</h3>
          <ul>
            <li><p>Generative Models</p></li>
            <li><p>3D Vision</p></li>
            <li><p>Multimodal</p></li>
          </ul>
        </div>

        <div class="section news-container">
          <h3>NEWS</h3>
          <ul>
            <li><p>[2025.05] One paper was accepted by ICCV 2025. [<a href="https://news.hrbeu.edu.cn/info/1015/86542.htm">News</a>]</p></li>
            <li><p>[2025.04] Selected as a Pioneer in Innovation and Entrepreneurship (Top 0.03%). [<a href="https://qihang.hrbeu.edu.cn/2024/0427/c960a324682/page.htm">News</a>]</p></li>
            <li><p>[2025.04] Received Outstanding Final Report Award for the National Undergraduate Training Program for Innovation and Entrepreneurship.</p></li>
            <li><p>[2025.03] New paper released! Align Your Rhythm: Generating Highly Aligned Dance Poses with Gating-Enhanced Rhythm-Aware Feature Representation.</p></li>
            <li><p>[2024.12] Awarded First Prize at the 9th National Youth Artificial Intelligence Innovation and Entrepreneurship Conference.</p></li>
            <li><p>[2024.11] FastDrag was selected as an Outstanding Achievement in the First Academic Annual Conference of Harbin Engineering University. [<a href="https://cstc.hrbeu.edu.cn/2024/1105/c3687a331657/page.htm">News</a>]</p></li>
            <li><p>[2024.09] One paper was accepted by NeurIPS 2024. [<a href="https://cstc.hrbeu.edu.cn/2024/0929/c3687a330582/page.htm">News</a>]</p></li>
            <li><p>[2024.09] Received a letter of appreciation from the Heilongjiang Provincial Department of Education.</p></li>
            <li><p>[2024.07] As the project leader, successfully led the application for the 2024 National Undergraduate Training Program for Innovation and Entrepreneurship (Top 4.16%).</p></li>
            <li><p>[2023.09] Participated in the 2023 Forum on Technological Innovation for the Maritime Power Strategy.</p></li>
            <li><p>[2023.08] Participated in the 18th National Conference on Computer-Supported Cooperative Work and Social Computing.</p></li>
          </ul>
        </div>

        <div class="section">
          <h2>Research</h2>
          <h3>Selected Publications</h3>
          <p>Thanks to all the collaborators for their great work! Check out <strong><a href="https://scholar.google.com/citations?user=bBNwgX8AAAAJ">Google Scholar</a></strong> for more.</p>

          <ul class="pubs">
            <li>
              <p class="authors"><i><span class="me">Congyi Fan</span>, Jian Guan, Xuanjia Zhao, Dongli Xu, Youtian Lin, Tong Ye, Haiwei Pan, Pengming Feng.</i></p>
              <p class="pub-title">Align Your Rhythm: Generating Highly Aligned Dance Poses with Gating-Enhanced Rhythm-Aware Feature Representation.</p>
              <p class="venue">In <i>IEEE/CVF International Conference on Computer Vision (ICCV), 2025.</i></p>
              <p class="links">[<a href="https://arxiv.org/abs/2503.17340">arXiv</a>] [<a href="https://danceba.github.io/">Demo</a>] [<a href="https://github.com/fancongyi/Danceba">Code</a>]</p>
            </li>

            <li>
              <p class="authors"><i>Xuanjia Zhao, Jian Guan, <span class="me">Congyi Fan</span>, Dongli Xu, Youtian Lin, Haiwei Pan, Pengming Feng.</i></p>
              <p class="pub-title">FastDrag: Manipulate Anything in One Step.</p>
              <p class="venue">In <i>Annual Conference on Neural Information Processing Systems (NeurIPS), 2024.</i></p>
              <p class="links">[<a href="https://arxiv.org/abs/2405.15769">arXiv</a>] [<a href="https://fastdrag-site.github.io/">Demo</a>] [<a href="https://github.com/XuanjiaZ/FastDrag">Code</a>]</p>
            </li>

            <li>
              <p class="authors"><i><span class="me">Congyi Fan*</span>, Shitong Fan*, Feiyang Xiao, Wenbo Wang, Xinyi Che, Qiaoxi Zhu, Jian Guan.</i></p>
              <p class="pub-title">GISP@HEU's Submission to the DCASE 2025 Challenge: Stereo SELD Task.</p>
              <p class="venue">In <i>DCASE 2025 Technical Report.</i></p>
              <p class="links">[<a href="https://dcase.community/documents/challenge2025/technical_reports/DCASE2025_Guan_77_t3.pdf">Paper</a>]</p>
            </li>
          </ul>
        </div>

        <div class="section">
          <h3>Experiences</h3>
          <p><strong>Research Intern</strong>, Guangdong Key Laboratory of New Security and Intelligence Technology, HITSZ, 2024.07â€“2024.11</p>
          <ul>
            <li><p>Led by <a href="https://faculty.hitsz.edu.cn/wangxuan">Xuan Wang</a> and supervised by <a href="http://csen.hitsz.edu.cn/info/1021/1962.htm">Shuhan Qi</a>.</p></li>
          </ul>
          <p><strong>Research Intern</strong>, State Key Laboratory of Space-Ground Integrated Information Technology (SGIIT), National Key Laboratory, 2025.06â€“Now</p>
          <ul>
            <li><p>Supervised by <a href="https://scholar.google.com/citations?user=O51mMKgAAAAJ">Pengming Feng</a> and <a href="https://scholar.google.com/citations?user=wf60G1sAAAAJ">Jian Guan</a>.</p></li>
          </ul>
        </div>

        <div class="section">
          <h3>Academic Service</h3>
          <p><strong>Conference Reviewer / TPC Member</strong></p>
          <ul>
            <li><p>IEEE International Conference on Multimedia &amp; Expo (ICME) 2024, 2025</p></li>
            <li><p>Chinese Conference on Pattern Recognition and Computer Vision (PRCV) 2025</p></li>
          </ul>
        </div>

        <div class="section">
          <h3>Competitions</h3>
          <ul>
            <li><p>Ranked 7th globally in Task 1 and 4th globally in Task 2 of the EEG-Music Emotion Recognition challenge at ICASSP 2024 Grand Challenge, 2024.01.</p></li>
            <li><p>1st in 9th National Youth Artificial Intelligence Innovation and Entrepreneurship Conference, 2024.12.</p></li>
            <li><p>3rd in Northeast Region National Undergraduate Information Security Competition, 2024.07.</p></li>
            <li><p>Honorable Mention in American Undergraduate Mathematical Contest in Modeling, 2024.05.</p></li>
            <li><p>Heilongjiang Province 1st in National Undergraduate Mathematical Contest in Modeling, 2023.11.</p></li>
          </ul>
          <p class="muted">(Total of 2 national awards and 7 provincial awards)</p>
        </div>

        <div class="section" style="text-align:center; margin-top: 28px;">
          <a href="https://clustrmaps.com/site/1bonu" title="Visit tracker"><img src="https://clustrmaps.com/map_v2.png?cl=ffffff&amp;w=300&amp;t=tt&amp;d=6yrMMIqIUJ1GT9nydxBO8cdtx5nw8iVTjA_d5hAcBcY&amp;co=2d78ad&amp;ct=ffffff" alt="Visitor map" /></a>
        </div>
      </td>
    </tr>
  </table>
</body>
</html>

<!--

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" href="favicon.ico" />
<link rel="icon" href="favicon.ico" type="image/x-icon" />
<title>Congyi Fan (èŒƒèªæ¯…)</title>
  
<style>
  body {
    font-family: "Times New Roman", Times, serif;
  }
  
  /* å…¨å±€ç¼©å°æ®µè½ä¸‹æ–¹é—´è· */
  p {
    margin-bottom: 6px;
    line-height: 1.5;
  }
  
  /* â†“ ç¼©å°åˆ—è¡¨é¡¹ä¸­çš„æ®µé—´è· â†“ */
  ul li p {
    margin-top: 4px;
    margin-bottom: 4px;
    line-height: 1.4;
  }
</style>

<style>
/* ç¼©å° publication åˆ—è¡¨é¡¹ä¹‹é—´çš„å‚ç›´é—´è· */
li p {
  margin-top: 4px;
  margin-bottom: 4px;
  line-height: 1.4;
}

/* å¯é€‰ï¼šè¿›ä¸€æ­¥æ§åˆ¶åˆ—è¡¨é¡¹æ•´ä½“å¤–è¾¹è·ï¼ˆå¦‚æœ <li> æœ¬èº«æœ‰ marginï¼‰ */
li {
  margin-bottom: 6px; /* æ§åˆ¶æ¯é¡¹ä¹‹é—´çš„é—´è· */
}
</style>
  
<style>
  /* æŠŠæ‰€æœ‰ <b> å†…çš„é“¾æ¥éƒ½è®¾æˆæ©™è‰² */
  b a {
    color: #224b8d;       
    text-decoration: underline;  /* ä¿ç•™ä¸‹åˆ’çº¿ */
  }
  /* ä½ è¿˜å¯ä»¥å®šä¹‰ hoverã€visited ç­‰çŠ¶æ€ */
  b a:hover {
    color: #3069C6;       /* é¼ æ ‡æ‚¬åœæ—¶ç¨å¾®äº®ä¸€ç‚¹ */
  }
  b a:visited {
    color: #843F9F;       /* è®¿é—®è¿‡åæ¢æˆæ·±ç´« */
  }
</style>
  
<style>
  a {
    color: #224b8d;
    text-decoration: underline;
  }
  a:hover {
    color: #3069C6;
  }
  a:visited {
    color: #843F9F;
  }
</style>

<style>
#layout-menu {
  background-color: #f8f9fa; /* æµ…ç°èƒŒæ™¯ */
  padding: 15px;
  border-right: 1px solid #ccc;
  width: 56px;
}
#layout-content {
  padding: 25px;
  background-color: #ffffff;
}
</style>
  
<style>
#tlayout {
  margin: 0 auto;
  max-width: 1500px;
  border-collapse: collapse;
}
</style>

<style>
h1, h2, h3, h4, h5, h6 {
  margin-top: 16px;
  margin-bottom: 6px;
}

li {
  margin-bottom: 4px;
}
</style>


</head>

<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="https://scholar.google.com/citations?user=bBNwgX8AAAAJ">Scholar</a></div>
<div class="menu-item"><a href="https://github.com/fancongyi">GitHub</a></div>
<!-- <div class="menu-item"><a href="https://www.linkedin.com/in/ziyang-ma">Linkedin</a></div> -->
<!-- <div class="menu-item"><a href="https://blog.csdn.net/weixin_45019478">Blog</a></div> -->
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Congyi Fan (èŒƒèªæ¯…)</h1>
</div>
<table class="imgtable">
  <tr>
    <td>
      <a href="index.html" style="text-decoration: none;">
        <img src="profile.png" alt="Congyi Fan's Avatar" width="150px" height="150px" style="border-radius: 8px; box-shadow: 2px 2px 6px rgba(0,0,0,0.2);" />
      </a>&nbsp;
    </td>
    <td align="left">
      <p>
        Undergraduate Student,<br />
        Harbin Engineering University,<br />
        Harbin, Heilongjiang Province, China.<br />
        <a href="mailto:fancongyi@hrbeu.edu.cn">fancongyi@hrbeu.edu.cn</a>
      </p>
    </td>
  </tr>
</table>



<h2>Biography</h2>
<p>HiğŸ‘‹ nice to meet you!</p>
<p>Currently I am an undergraduate student majoring in Computer Science and Technology (Information Security) at Harbin Engineering University (HEU), serving as a research assistant to Associate Professor <a href="https://scholar.google.com/citations?user=wf60G1sAAAAJ">Guan Jian</a>.</p>
<p>My research is jointly supervised by Associate Professor Guan Jian at HEU, Researcher <a href="https://scholar.google.com/citations?user=O51mMKgAAAAJ">Pengming Feng</a> from the State Key Laboratory of Space-Ground Integrated Information Technology (SGIIT), and Dr. <a href="https://scholar.google.com/citations?user=H56DRXwAAAAJ">Qiaoxi Zhu</a> from the University of Technology Sydney.<p>
<p>My research is guided by the aspiration to advance scientific and technological progress. My recent work focuses on <b>Generative Models, 3D Vision and Multimodal</b>. If you are also interested, please feel free to contact me.</p>

<h3>Education</h3>
<ul>
<!-- <li><p>Ph.D., Computer Science and Engineering, Shanghai Jiao Tong University, 2022.09-Now</p>
</li>
<li><p>Ph.D., Computer Science and Engineering, Nanyang Technological University, 2022.09-Now</p>
</li> -->
<li><p>B.E., Information Security, Harbin Engineering University, 2022.09-Now</p>
</li>
</ul>

<h3>Interests</h3>
<ul>
<li><p>Generative Models</p>
</li>
<li><p>3D Vision</p>
</li>
<li><p>Multimodal</p>
<!-- </li>
<li><p>Multimodal Reasoning</p> -->
</li>
</ul>

<style>
/* NEWSå®¹å™¨æ ·å¼ */
.news-container {
    max-height: 400px;       /* è®¾ç½®æœ€å¤§é«˜åº¦ */
    overflow-y: auto;        /* å‚ç›´æ–¹å‘è‡ªåŠ¨æ˜¾ç¤ºæ»šåŠ¨æ¡ */
    padding-right: 15px;     /* ä¸ºæ»šåŠ¨æ¡é¢„ç•™ç©ºé—´ */
    margin: 20px 0;          /* æ·»åŠ å¤–è¾¹è· */
    border: 1px solid #eee;  /* å¯é€‰è¾¹æ¡† */
    border-radius: 8px;      /* åœ†è§’ */
}

/* è‡ªå®šä¹‰æ»šåŠ¨æ¡æ ·å¼ */
.news-container::-webkit-scrollbar {
    width: 6px;              /* æ»šåŠ¨æ¡å®½åº¦ */
}
.news-container::-webkit-scrollbar-track {
    background: #f1f1f1;     /* è½¨é“é¢œè‰² */
    border-radius: 3px;
}
.news-container::-webkit-scrollbar-thumb {
    background: #888;        /* æ»‘å—é¢œè‰² */
    border-radius: 3px;
}
.news-container::-webkit-scrollbar-thumb:hover {
    background: #555;        /* é¼ æ ‡æ‚¬åœé¢œè‰² */
}

/* è°ƒæ•´åŸæœ‰æ ·å¼ */
.news-container h3 {
    margin: 15px 0;
    position: sticky;        /* å›ºå®šæ ‡é¢˜ */
    top: 0;
    background: white;
    z-index: 1;
    padding: 10px 0;
}
.news-container ul {
    margin: 0;
    padding: 0 0 15px 25px;  /* è°ƒæ•´åˆ—è¡¨å†…è¾¹è· */
}
</style>

<!-- HTMLç»“æ„ -->
<div class="news-container">
<h3>NEWS</h3>
<!-- <img src="pic/new_gif.gif" alt="alt text"/> -->
<ul>
<li><p>[2025.05] 1 papers was accpeted by ICCV 2025. [<a href="https://news.hrbeu.edu.cn/info/1015/86542.htm">News</a>]</p></li>
<li><p>[2025.04] Selected as a Pioneer in Innovation and Entrepreneurship (Top 0.03%). [<a href="https://qihang.hrbeu.edu.cn/2024/0427/c960a324682/page.htm">News</a>]</p></li>
<li><p>[2025.04] Received Outstanding Final Report Award for the National Undergraduate Training Program for Innovation and Entrepreneurship.</p></li>
<li><p>[2025.03] New paper released! Align Your Rhythm: Generating Highly Aligned Dance Poses with Gating-Enhanced Rhythm-Aware Feature Representation.</p></li>
<li><p>[2024.12] Awarded First Prize at the 9th National Youth Artificial Intelligence Innovation and Entrepreneurship Conference.</p></li>
<li><p>[2024.11] FastDrag was successfully selected as an Outstanding Achievement in the First Academic Annual Conference of Harbin Engineering University. [<a href="https://cstc.hrbeu.edu.cn/2024/1105/c3687a331657/page.htm">News</a>]</p></li>
<li><p>[2025.05] 1 papers was accpeted by NeurIPS 2024. [<a href="https://cstc.hrbeu.edu.cn/2024/0929/c3687a330582/page.htm">News</a>]</p></li>
<li><p>[2024.09] I have received a letter of appreciation from the Heilongjiang Provincial Department of Education.</p></li>
<li><p>[2024.07] As the project leader, successfully led the application for the 2024 National Undergraduate Training Program for Innovation and Entrepreneurship (Top 4.16%).</p></li>
<li><p>[2023.09] Participated in the 2023 Forum on Technological Innovation for the Maritime Power Strategy.</p></li>
<li><p>[2023.08] Participated in the 18th National Conference on Computer-Supported Cooperative Work and Social Computing.</p></li>
<!-- <li><p> </p></li>
<li><p> </p></li>
<li><p> </p></li>
<li><p> </p></li>
<li><p> </p></li>
<li><p> </p></li> -->
</ul>
</div>


<h2>Research</h2>
<h3>Selected Publications</h3>
<p>Thanks to all the collaborators for their great work!</p>
<p>Check out <b><a href="https://scholar.google.com/citations?user=bBNwgX8AAAAJ">Google Scholar</a></b> for more information.</p>

<!-- <p><b>Speech, Language, Audio, Music Processing with SSL</b></p> -->
<ul>

<li><p>
<i><span style="color:#224b8d"><b>Congyi Fan</b></span>, Jian Guan, Xuanjia Zhao, Dongli Xu, Youtian Lin, Tong Ye, Haiwei Pan, Pengming Feng
.</i><br>
<b>Align Your Rhythm: Generating Highly Aligned Dance Poses with Gating-Enhanced Rhythm-Aware Feature Representation.</b><br>
in <i>IEEE/CVF International Conference on Computer Vision (ICCV), 2025.</i> [<a href="https://arxiv.org/abs/2503.17340">arXiv</a>][<a href="https://danceba.github.io/">Demo</a>][<a href="https://github.com/fancongyi/Danceba">Code</a>]<br>
</p></li>
<li><p>

<i>Xuanjia Zhao, Jian Guan, <span style="color:#224b8d"><b>Congyi Fan</b></span>, Dongli Xu, Youtian Lin, Haiwei Pan, Pengming Feng
.</i><br>
<b>FastDrag: Manipulate Anything in One Step.</b><br>
in <i>Annual Conference on Neural Information Processing Systems (NeurIPS), 2024.</i> [<a href="https://arxiv.org/abs/2405.15769">arXiv</a>][<a href="https://fastdrag-site.github.io/">Demo</a>][<a href="https://github.com/XuanjiaZ/FastDrag">Code</a>]<br>
</p></li>
<li><p>

<i><span style="color:#224b8d"><b>Congyi Fan*</b></span>, Shitong Fan*, Feiyang Xiao, Wenbo Wang, Xinyi Che, Qiaoxi Zhu, Jian Guan
.</i><br>
<b>GISP@HEU's Submission to the DCASE 2025 Challenge: Stereo SELD Task.</b><br>
in <i>Dcase 2025 Technical Report.</i> [<a href="https://dcase.community/documents/challenge2025/technical_reports/DCASE2025_Guan_77_t3.pdf">Paper</a>] <br>
</p></li>

</ul>

<!-- <p>Note: * indicates the corresponding author.</p> -->
<!-- <p><a href="https://scholar.google.com/citations?user=4RZnXGMAAAAJ&hl=zh-CN&oi=ao">Full list of publications in Google Scholar</a>.</p> -->

<h3>Experiences</h3>
<p>Research Intern, Guangdong Key Laboratory of New Security and Intelligence Technology, HITSZ, 2024.07-2024.11</p>
<ul>
<!-- <li><p>Investigate full-duplex modeling for speech interaction and dialog system.</p>
</li> -->
<li><p>Led by <a href="https://faculty.hitsz.edu.cn/wangxuan">Xuan Wang</a> and Supervised by <a href="http://csen.hitsz.edu.cn/info/1021/1962.htm">Shuhan Qi</a>.</p>
</li>
</ul> 
<p>Research Intern, State Key Laboratory of Space-Ground Integrated Information Technology (SGIIT), National Key Laboratory, 2025.06-now</p>
<ul>
<!-- <li><p>Investigate emotional speech dialogue system with large language model.</p>
</li> -->
<li><p>Supervised by <a href="https://scholar.google.com/citations?user=O51mMKgAAAAJ">Pengming Feng</a> and <a href="https://scholar.google.com/citations?user=wf60G1sAAAAJ">Jian Guan</a>
</li>
</ul> 
</ul>

<h3>Academic Service</h3>
<!-- <p><b>Organizing Committee / Chair</b></p>
<ul>
<li><p>Multimodal Emotion Recognition Challenge (MER24) @ACM Multimedia MRAC24 Workshop</p>
</li>
</ul> -->
<p><b>Conference Reviewer / TPC Member</b></p>
<ul>
<li><p>IEEE International Conference on Multimedia & Expo (ICME) 2024, 2025</p>
</li>    
<li><p>Chinese Conference on Pattern Recognition and Computer Vision (PRCV) 2025</p>
</li>
</ul>
<!-- <p><b>Journal Reviewer</b></p>
<ul>
<li><p>IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCAVT)</p>
</li> -->
</ul>
<!-- <p><a href="https://publons.com/researcher/3034188/xiuze-zhou/">More details in Publons</a></p> -->

<!-- <h2>Open-Source Projects</h2>
<h3>Projects</h3>
<p><b>SLAM-LLM</b>[<a href="https://github.com/ddlBoJack/SLAM-LLM">GitHub</a>]</p>
<ul>
<li><p>SLAM-LLM is a deep learning toolkit that allows researchers and developers to train custom multimodal large language model (MLLM), focusing on Speech, Language, Audio, Music processing.</p>
</li>
</ul>
<p><b>FunAudioLLM</b>[<a href="https://github.com/FunAudioLLM">GitHub</a>][<a href="https://fun-audio-llm.github.io/pdf/FunAudioLLM.pdf">Techinical Report</a>][<a href="https://huggingface.co/FunAudioLLM">HuggingFace</a>][<a href="https://fun-audio-llm.github.io/">Demo</a>]</p>
<ul>
<li><p><b>SenseVoice</b> is a speech foundation model with multiple speech understanding capabilities.[<a href="https://github.com/FunAudioLLM/SenseVoice">GitHub</a>][<a href="https://www.modelscope.cn/models/iic/SenseVoiceSmall">ModelScope</a>]</p>
</li>
<li><p><b>CosyVoice</b> is a multi-lingual large voice generation model.[<a href="https://github.com/FunAudioLLM/CosyVoice">GitHub</a>][<a href="https://www.modelscope.cn/studios/iic/CosyVoice-300M">ModelScope</a>]</p>
</li>
</ul>
<p><b>emotion2vec series</b>[<a href="https://github.com/ddlBoJack/emotion2vec">GitHub</a>][<a href="https://arxiv.org/abs/2312.15185">emotion2vec(ACL2024)</a>][<a href="https://huggingface.co/emotion2vec">HuggingFace</a>][<a href="https://modelscope.cn/models?name=emotion2vec">ModelScope</a>]</p>
<ul>
<li><p><b>emotion2vec</b> is the first universal speech emotion representation model.</p>
</li>
<li><p><b>emotion2vec+</b> is a series of foundational models for speech emotion recognition (SER).</p>
</li>
</ul>
<p><b>MAP-Neo series</b>[<a href="https://github.com/multimodal-art-projection/MAP-NEO">GitHub</a>][<a href="https://arxiv.org/abs/2405.19327">Techinical Report</a>][<a href="https://huggingface.co/collections/m-a-p/neo-models-66395a5c9662bb58d5d70f04">HuggingFace</a>]</p>
<ul>
<li><p><b>MAP-Neo</b> is a series of fully open-sourced large language models.</p>
</li>
<li><p><b>Matrix</b> is the pretraining data and data processing pipeline for MAP-Neo.[<a href="https://huggingface.co/datasets/m-a-p/Matrix">Dataset</a>]</p>
</li>
</ul> -->

<!-- <h2>Accomplishments</h2>
<h3>Awards</h3>
<ul>
<li><p>SPS Travel Grant, IEEE, 2024.02</p>
</li>
<li><p>Best Presentation Award in Student Forum, the 18th National Conference on Man-Machine Speech Communication (NCMMSC), 2023.12</p>
</li>
<li><p>Interspeech Best Student Paper Shortlist, ISCA, 2023.08</p>
</li>
<li><p>Excellent Graduate, Department of Education, Shandong Province, China, 2022.06</p>
</li>
<li><p>"Intelligent Pedestal" Scholarship, Huawei, 2021.12</p>
</li>
<li><p>SIGMM Student Travel Grant, ACM, 2021.11</p>
</li>
<li><p>National Scholarship, Ministry of Education, China, 2021.10</p>
</li>
</ul> -->

<h3>Competitions</h3>
<ul>
<li><p>Ranked 7th globally in Task 1 and 4th globally in Task 2 of the EEG-Music Emotion Recognition challenge at ICASSP 2024 Grand Challenge, 2024.01.</p></li>
<li><p>1st in 9th National Youth Artificial Intelligence Innovation and Entrepreneurship Conference, 2024.12. </p></li>
<li><p>3rd in Northeast Region National Undergraduate Information Security Competition, 2024.07.</p></li>
<li><p>Honorable Mention in American Undergraduate Mathematical Contest in Modeling, 2024.05.</p></li>
<li><p>Heilongjiang Province 1st in National Undergraduate Mathematical Contest in Modeling, 2023.11.</p></li>
(Total of 2 national awards and 7 provincial awards)
</ul>

<!-- <h3>Activities</h3>
<ul>
<li><p>Invited Talk: Towards Interactive Speech Language Model, Nvidia, 2024.10</p></li>
<li><p>Invited Talk: Towards Interactive Speech Language Model, The Hong Kong University of Science and Technology(HKUST), 2024.8</p></li>
<li><p>Invited Talk: Speech & Audio Understanding Based on SSL and LLM, Nvidia, 2024.6</p></li>
<li><p>Invited Talk: INTERSPEECH 2023 Pre-presentation, SpeechHome, 2023.07</p></li>
<li><p>Invited Talk: Towards More Realistic, Powerful, and Accurate Speech-based Self-Supervised Learning </a>, The Renmin University of China(RUC), 2023.5</p></li>
<li><p>PhD Debate Towards AIGC, AI TIME, 2023.1</p></li>
<li><p>[<a href="https://www.bilibili.com/video/BV1iV4y137RC">Invited Talk</a>]: How to conduct audio-driven talking head? An introduction and solution sharing, Datawhale, 2022.11</p></li>
<li><p>Member of <a href="https://datawhale.club/">Datawhale</a>, 2022.09-Now</p></li>
<li><p>Teaching Assistant, Computer Science and Technology, Shandong University, 2021.03-2021.06</p></li>
<li><p>Member of Elite Class, Computer Science and Technology, Shandong University, 2020.09-2022.06</p></li>
</ul> -->

<!-- <p><br><a href="cv/cv.pdf">A brief cv</a>.</p> -->
<center>
<a href="https://clustrmaps.com/site/1bonu" title="Visit tracker"><img src="https://clustrmaps.com/map_v2.png?cl=ffffff&w=300&t=tt&d=6yrMMIqIUJ1GT9nydxBO8cdtx5nw8iVTjA_d5hAcBcY&co=2d78ad&ct=ffffff" /></a>
</center>
</td>
</tr>
</table>
</body>
</html>

-->
